<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【学习】手搓Transformer：Multi-Attention</title>
      <link href="/2025/03/13/xue-xi-transformer-yuan-li-er/"/>
      <url>/2025/03/13/xue-xi-transformer-yuan-li-er/</url>
      
        <content type="html"><![CDATA[<h2 id="手搓Transformer：Multi-Attention"><a href="#手搓Transformer：Multi-Attention" class="headerlink" title="手搓Transformer：Multi-Attention"></a>手搓Transformer：Multi-Attention</h2><hr><blockquote><p>《Attention Is All You Need》里的Transformer架构</p></blockquote><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/1741076487595.jpg?imageSlim" alt="Transformer" style="zoom:55%;" /><h4 id="1-Self-Attention"><a href="#1-Self-Attention" class="headerlink" title="1. Self-Attention"></a>1. Self-Attention</h4><p>自注意力的作用：随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。在处理过程中，自注意力机制会将对所有相关单词的理解融入到我们正在处理的单词中。更具体的功能如下：</p><p><strong>序列建模：</strong>自注意力可以用于序列数据（例如文本、时间序列、音频等）的建模。它可以捕捉序列中不同位置的依赖关系，从而更好地理解上下文。这对于机器翻译、文本生成、情感分析等任务非常有用。<br><strong>并行计算：</strong>自注意力可以并行计算，这意味着可以有效地在现代硬件上进行加速。相比于RNN和CNN等序列模型，它更容易在GPU和TPU等硬件上进行高效的训练和推理。（因为在自注意力中可以并行的计算得分）<br><strong>长距离依赖捕捉：</strong>传统的循环神经网络（RNN）在处理长序列时可能面临梯度消失或梯度爆炸的问题。自注意力可以更好地处理长距离依赖关系，因为它不需要按顺序处理输入序列。</p><p><strong>自注意力的计算：</strong></p><p>Self-Attention 是通过输入本身的序列作为 Q 和 K 计算注意力分数的。从每个编码器的输入向量（每个单词的词向量，即Embedding，可以是任意形式的词向量，比如说word2vec，GloVe，one-hot编码）中生成三个向量，即查询向量、键向量和一个值向量。（这三个向量是通过词嵌入与三个权重矩阵相乘后创建出来的）</p><blockquote><p>参考 CSDN @告白气球 的这张图：</p></blockquote><img src= "https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/cc00beb97c344a486d07e3d9e8a58f06.png?imageSlim"><p>经过上一节 Embedding 层后的嵌入输出维度为 <strong>(batch_size, seq_len, d_model)，</strong>通过 nn.Linear 线性变换为 k, q, v 向量。然后经过注意力分数计算的公式即可得到注意力 score：q 和 k 先相乘，得到 (batch_size, num_heads, seq_len, seq_len) 的score矩阵，矩阵表示序列中每个位置对所有其他位置的注意力权重；softmax 之后与 v 相乘，score 的形状变为 **[batch_size, num_heads, seq_len, n_d]**。</p><h4 id="2-Multi-Head"><a href="#2-Multi-Head" class="headerlink" title="2. Multi-Head"></a>2. Multi-Head</h4><p><strong>self-attention</strong>只是使用了一组 WQ、WK、WV 来进行变换得到查询、键、值矩阵，而 Multi-Head Attention 使用多组WQ，WK，WV得到多组查询、键、值矩阵，然后每组分别计算得到一个 Z 矩阵。</p><p>将所有注意力头 Z 矩阵拼接起来，得到的就是合并了所有头的注意力信息。展平 num_heads 维度，得到 <strong>[batch_size, seq_len, d_model]</strong> 形状的 score。</p><p>最后用一个附加的权重矩阵 W^O 乘以 score，将展平的score通过线性层投影回 d_model 维度，并加上偏置。这个线性变换将合并后的表示投影到一个更有意义的空间。</p><p>总结整个流程就是：</p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/0b8dacfc201e24ef7dc0e690b41b998c.png?imageSlim"><p>最后附上带有详细注释的代码，流程应该很清楚：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> data<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> random<span class="token keyword">import</span> math<span class="token keyword">from</span> torch <span class="token keyword">import</span> Tensor<span class="token comment" spellcheck="true"># batch_size, seq_len, d_model</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">:</span> int<span class="token punctuation">,</span> num_heads<span class="token punctuation">:</span> int<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        <span class="token comment" spellcheck="true"># 线性层的权重矩阵形状为(d_model, d_model)</span>        <span class="token comment" spellcheck="true"># 输入的形状为(batch_size, seq_len, d_model)，线性变换(wx+b)后得到k,q,v</span>        <span class="token comment" spellcheck="true"># 不过一般新向量在维度上往往比词嵌入向量更低</span>        <span class="token comment" spellcheck="true"># W_q：学习"我应该关注什么"的特征表示</span>        <span class="token comment" spellcheck="true"># W_k：学习"我能提供什么信息"的特征表示</span>        <span class="token comment" spellcheck="true"># W_v：学习"我的实际内容是什么"的特征表示</span>        self<span class="token punctuation">.</span>w_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w_o <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> d_model <span class="token operator">=</span> q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        n_d <span class="token operator">=</span> self<span class="token punctuation">.</span>d_model <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads        <span class="token comment" spellcheck="true"># 将输入向量x线性变换为q,k,v</span>        q<span class="token punctuation">,</span>k<span class="token punctuation">,</span>v <span class="token operator">=</span> self<span class="token punctuation">.</span>w_q<span class="token punctuation">(</span>q<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>w_k<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>w_v<span class="token punctuation">(</span>v<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 这里通过view操作将d_model维度拆分为num_heads个头，每个头的大小为n_d</span>        <span class="token comment" spellcheck="true"># 然后通过permute操作将头维度移到前面，变成(batch_size, num_heads, seq_len, n_d)</span>        <span class="token comment" spellcheck="true"># 目的：1.减少计算量，并行计算；2.每个头可以关注不同特征子空间</span>        q <span class="token operator">=</span> q<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> n_d<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>        k <span class="token operator">=</span> k<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> n_d<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>        v <span class="token operator">=</span> v<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> n_d<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 这里k.transpose(2,3)将k的维度从(batch_size, num_heads, seq_len, n_d)变为(batch_size, num_heads, n_d, seq_len)</span>        <span class="token comment" spellcheck="true"># 这样q和k相乘后，得到(batch_size, num_heads, seq_len, seq_len)的score矩阵</span>        <span class="token comment" spellcheck="true"># 矩阵表示序列中每个位置对所有其他位置的注意力权重</span>        <span class="token comment" spellcheck="true"># score[b, h, i, j] 表示在批次 b 的第 h 个头中，位置 i 对位置 j 的注意力分数</span>        score <span class="token operator">=</span> q@k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>n_d<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 将 mask 中值为0的位置在 score 中填充为一个很大的负数 -1e9</span>        <span class="token comment" spellcheck="true"># 在编码器自注意力中，通常不使用掩码，允许每个位置关注所有位置</span>        <span class="token comment" spellcheck="true"># 在解码器自注意力中，使用掩码确保每个位置只能关注其前面的位置</span>        <span class="token comment" spellcheck="true"># 在填充掩码中，用于忽略填充标记的影响</span>        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            score <span class="token operator">=</span> score<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># self.softmax(score) 将注意力分数转换为概率分布（每行和为1）</span>        <span class="token comment" spellcheck="true"># score 的形状变为 [batch_size, num_heads, seq_len, n_d]</span>        score <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>score<span class="token punctuation">)</span>@v        <span class="token comment" spellcheck="true"># 将 score 的维度转换为 [batch_size, seq_len, num_heads, n_d]</span>        <span class="token comment" spellcheck="true"># 然后展平 num_heads 维度，得到 [batch_size, seq_len, d_model]</span>        score <span class="token operator">=</span> score<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 将展平的score通过线性层投影回d_model维度，并加上偏置</span>        <span class="token comment" spellcheck="true"># 这个线性变换将合并后的表示投影到一个更有意义的空间</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>w_o<span class="token punctuation">(</span>score<span class="token punctuation">)</span>        <span class="token keyword">return</span> out</code></pre>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】手搓Transformer：Embedding</title>
      <link href="/2025/02/23/xue-xi-transformer-yuan-li-yi/"/>
      <url>/2025/02/23/xue-xi-transformer-yuan-li-yi/</url>
      
        <content type="html"><![CDATA[<h2 id="手搓Transformer：Embedding"><a href="#手搓Transformer：Embedding" class="headerlink" title="手搓Transformer：Embedding"></a>手搓Transformer：Embedding</h2><hr><blockquote><p>《Attention Is All You Need》里的Transformer架构</p></blockquote><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/1741076487595.jpg?imageSlim" alt="Transformer" style="zoom:55%;" /><p>先从 Transformer 的 Embedding 部分写起。当我们输入文本到 Transformer 网络中时，模型接收的是存储着数值的向量而不是文字，因而第一步要做的事就是将这个 text 转化为词向量。</p><h4 id="1-Tokenizer-处理得到词索引"><a href="#1-Tokenizer-处理得到词索引" class="headerlink" title="1. Tokenizer 处理得到词索引"></a>1. Tokenizer 处理得到词索引</h4><p>假定我们输入的 text 文本是一个包含许多单词的句子，我们需要想办法用数值表达这个句子，一种做法就是对每一个单词取一个唯一的索引值，用这个数值来表示这个句子。这些索引映射关系存起来就是<strong>词表。</strong></p><ul><li>当然分词粒度也不一定就是一个单词。只是说单词分词法（word base）最直观。<strong>单词分词法</strong>将一个word作为最小元，也就是根据空格或者标点分词。最详尽的分词是<strong>单字分词法</strong>（character-base）。单字分词法会穷举所有出现的字符，所以是最完整的。另外还有一种最常用的、介于两种方法之间的分词法叫<strong>子词分词法</strong>，会把一个句子分成最小可分的子词例如[‘To’, ‘day’, ‘is’, ‘S’, ‘un’, ‘day’]。</li></ul><p>比如 text &#x3D; “Today is a nice day!”，token &#x3D; tokenizer(text)，那么处理后得到的 tokens 就是一个字典，其中包含了 “input_ids” 词索引序列：’input_ids’: [[101, 2769, 3221, …, 102, 0, 0]]。0 是 padding 的填充项（补全 seq length 长度）</p><h4 id="2-Token-Embedding-词嵌入处理"><a href="#2-Token-Embedding-词嵌入处理" class="headerlink" title="2. Token_Embedding 词嵌入处理"></a>2. Token_Embedding 词嵌入处理</h4><p>经过 tokenizer 分词处理后，我们的输入 x 就变为一个形状 <strong>[batch_size, seq_len]</strong> 的向量，例如 [[101, 2054, 2003, 2026, 3793, 102], [101, 2054, 2064, 2017, 102, 0]] 表示输入 batch_size &#x3D; 2 两个句子。</p><p>那么 embedding 操作就是把输入向量升维。我们输入的 x 对一个单词的表示现在相当于是一个索引值，而我们要把每一个单词都变成一个唯一的向量模型才能学习到其空间嵌入特征。具体而言，假设我们有以下配置：</p><ul><li><p>批次大小 (batch_size) &#x3D; 2</p></li><li><p>序列长度 (seq_len) &#x3D; 4</p></li><li><p>嵌入维度 (d_model) &#x3D; 6</p></li><li><p>词汇表大小 (vocab_size) &#x3D; 10000</p></li></ul><pre class=" language-python"><code class="language-python">x <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">2054</span><span class="token punctuation">,</span> <span class="token number">2003</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 第一个句子的词索引</span>    <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">5243</span><span class="token punctuation">,</span> <span class="token number">3122</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># 第二个句子的词索引</span><span class="token punctuation">]</span></code></pre><p>嵌入后的张量可能就是这样，形状变为 [2,4,6] <strong>(batch_size, seq_len, d_model)</strong></p><pre class=" language-python"><code class="language-python">token_embedding <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token punctuation">[</span>  <span class="token comment" spellcheck="true"># 第一个句子</span>        <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 词索引101的嵌入向量</span>        <span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 词索引2054的嵌入向量</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 词索引2003的嵌入向量</span>        <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># 词索引102的嵌入向量</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span>  <span class="token comment" spellcheck="true"># 第二个句子</span>        <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 词索引101的嵌入向量</span>        <span class="token punctuation">[</span><span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 词索引5243的嵌入向量</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 词索引3122的嵌入向量</span>        <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># 词索引102的嵌入向量</span>    <span class="token punctuation">]</span><span class="token punctuation">]</span></code></pre><p>嵌入转化的原理实际就是一个查找表。我们首先把每个单词的词索引转化成 one-hot 向量，那么可以想象这个向量维数应该很大，因此再降维。从每个单词的 one-hot 向量经过 <strong>Embedding 矩阵</strong>得到降维后的结果。比如有三个单词， one-hot 处理后的结果是 $3<em>6$ 大小的向量，那么经过一个 $6</em>4$ 大小的权重矩阵就可以乘出 $3*4$ 大小的结果从而降维（d_model &#x3D; 4）。因此这个嵌入矩阵的形状就应该是[vocab_size, d_model] 大小。</p><p>继承 torch 里的 Embedding 类，forward 的时候把输入 x 转换为嵌入向量。具体实现：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 将输入的词汇表索引转换为指定维度的Embedding</span><span class="token comment" spellcheck="true"># 每个token的词索引升维到d_model维，padding_idx=1表示填充词的索引为1</span><span class="token comment" spellcheck="true"># 继承nn.Embedding在训练中前向传播，反向传播，更新参数</span><span class="token keyword">class</span> <span class="token class-name">TokenEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>TokenEmbedding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><h4 id="2-Position-Embedding-位置编码"><a href="#2-Position-Embedding-位置编码" class="headerlink" title="2. Position_Embedding 位置编码"></a>2. Position_Embedding 位置编码</h4><p>Transformer 架构引入了位置编码，为每个单词向量生成一个固定的位置向量，来表达其位置相对关系。</p><p>这部分是固定计算的，对于每个 seq_len 长度的句子输入，会返回一个 <strong>[seq_len, d_model]</strong> 形状即符合当前序列长度的位置编码。把词嵌入和位置编码相加才是最终的词向量。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 通过位置编码计算输入每个词生成的正弦余弦位置编码</span><span class="token comment" spellcheck="true"># 创建的是固定不变的位置编码，在训练中不更新，直接基于公式计算这个序列长度的位置编码矩阵</span><span class="token keyword">class</span> <span class="token class-name">PositionalEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>PositionalEmbedding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 初始化一个大小为(max_len, d_model)的零矩阵</span>        self<span class="token punctuation">.</span>encoding <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoding<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>        pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>        pos <span class="token operator">=</span> pos<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        _2i <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> step<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>pos <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">10000</span> <span class="token operator">**</span> <span class="token punctuation">(</span>_2i <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>pos <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">10000</span> <span class="token operator">**</span> <span class="token punctuation">(</span>_2i <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># x 形状: [batch_size, seq_len]，也就是词索引</span>        batch_size<span class="token punctuation">,</span> seq_len <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 返回适合当前序列长度的位置编码</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoding<span class="token punctuation">[</span><span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span></code></pre><h4 id="3-Transformer-Embedding"><a href="#3-Transformer-Embedding" class="headerlink" title="3. Transformer_Embedding"></a>3. Transformer_Embedding</h4><p>最后把词嵌入和位置编码合并，即直接相加。（这里在嵌入层神经网络应用一个 dropout 防止过拟合）</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 嵌入层的输入是经过tokenizer处理后的词索引，输出是词的Embedding和位置编码</span><span class="token keyword">class</span> <span class="token class-name">TransformerEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> drop_prob<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>TransformerEmbedding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>token_embedding <span class="token operator">=</span> TokenEmbedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>positional_embedding <span class="token operator">=</span> PositionalEmbedding<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> device<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>drop_prob<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        token_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>token_embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        positional_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>positional_embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>token_embedding <span class="token operator">+</span> positional_embedding<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】手写一个lora微调算法~</title>
      <link href="/2025/02/22/xue-xi-shou-xie-yi-ge-lora-wei-diao-suan-fa/"/>
      <url>/2025/02/22/xue-xi-shou-xie-yi-ge-lora-wei-diao-suan-fa/</url>
      
        <content type="html"><![CDATA[<h2 id="手写一个LoRA算法"><a href="#手写一个LoRA算法" class="headerlink" title="手写一个LoRA算法"></a>手写一个LoRA算法</h2><blockquote><p>参考视频： <a href="https://www.bilibili.com/video/BV1ZZ421T7XJ/?share_source=copy_web&vd_source=2ff04f0cc70237133af9bcd6be27a652">https://www.bilibili.com/video/BV1ZZ421T7XJ/?share_source=copy_web&amp;vd_source=2ff04f0cc70237133af9bcd6be27a652</a></p></blockquote><hr><h3 id="初始化LoRA-Linear类"><a href="#初始化LoRA-Linear类" class="headerlink" title="初始化LoRA Linear类"></a>初始化LoRA Linear类</h3><p>首先 LoRA 的原理就是在预训练模型的权重矩阵上添加两个 LoRA_A 和 LoRA_B 矩阵组合输出嘛，由于 LoRA_A 和 LoRA_B 矩阵可以通过秩 $r$ 来调整规模，因此训练起来参数量就比 FT 小了很多。</p><p>所以实际上在训练的时候，我们是通过冻结原模型参数矩阵，来学 LoRA_A 和 LoRA_B 矩阵的参数来实现的。那么就需要将原模型里的 Linear 层给替换成封装了 LoRA 模块的 LoRA Linear 层。</p><p>LoraLinear 类的初始化如下：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LoraLinear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">_init_</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> merge<span class="token punctuation">,</span> rank<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>LoraLinear<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>_init_<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features        self<span class="token punctuation">.</span>merge <span class="token operator">=</span> merge        self<span class="token punctuation">.</span>rank <span class="token operator">=</span> rank        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> alpha        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> dropout</code></pre><p><code>in_features</code>：输入维度，比如词嵌入维度是768，这里就是768；</p><p><code>out_features</code>：输出维度;</p><p><code>merge</code>：控制是否合并 LoRA 和原始权重;</p><p><code>rank</code>：LoRA 的秩，越小压缩程度越大，控制 LoRA 矩阵参数量的主要因素;</p><p><code>alpha</code>：缩放系数，用于调节 LoRA 矩阵参数的影响程度;</p><p><code>dropout</code>：防止过拟合的 dropout 率;</p><pre class=" language-python"><code class="language-python">        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">)</span>        <span class="token keyword">if</span> rank <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 构建权重参数</span>            self<span class="token punctuation">.</span>lora_b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> rank<span class="token punctuation">)</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>lora_a <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>rank<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>alpha <span class="token operator">/</span> rank            self<span class="token punctuation">.</span>linear<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token keyword">if</span> dropout <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>initial_weights<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">initial_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_a<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>创建 LoRA 的 A 和 B 矩阵：A (rank × in_features) 和 B (out_features × rank)；</p><p>计算缩放因子 scale &#x3D; alpha &#x2F; rank；</p><p>使用 Kaiming 初始化来初始化 LoRA 矩阵 A，这里只需要初始化一个权重矩阵，另一个 B 矩阵初始化为零矩阵。这样训练开始时 A @ B &#x3D; zeros，随着训练进行会慢慢调整 B 矩阵的参数值；</p><h3 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h3><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>rank <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">and</span> self<span class="token punctuation">.</span>merge<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 可以看到这边lora_b是out_features * rank的矩阵，lora_a是rank * in_features的矩阵，乘完后是out_features * in_features的矩阵</span>            <span class="token comment" spellcheck="true"># 那么weight就是out_features * in_features的矩阵，刚好和linear.weight的形状一样</span>            <span class="token comment" spellcheck="true"># 而wx+b，因此x就是in_features * batch_size的矩阵</span>            output <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> weight<span class="token operator">=</span>self<span class="token punctuation">.</span>linear<span class="token punctuation">.</span>weight <span class="token operator">+</span> self<span class="token punctuation">.</span>lora_b @ self<span class="token punctuation">.</span>lora_a <span class="token operator">*</span> self<span class="token punctuation">.</span>scale<span class="token punctuation">,</span> bias<span class="token operator">=</span>self<span class="token punctuation">.</span>linear<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>            output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>output<span class="token punctuation">)</span>            <span class="token keyword">return</span> output        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><p>这里就是用 LoRA 的 A 和 B 矩阵乘上原始权重矩阵的计算过程来在前向中参与进计算过程。在训练的时候就会反向更新 LoRA 两个矩阵的参数。</p><p>最简单的实现过程就是这样，假若要将 LoRA 模块包装到具体的一个模型进行微调，那么可以把 base_model 里的 module 里面的线性层替换成 LoRA Linear 层，然后再冻结预训练权重在下游任务上进行训练。可以直接利用 hugging face 的 Trainer 类开一个 Trainer 直接训。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】在DeepSeek-R1-1.5b模型上做一个简单的lora微调~</title>
      <link href="/2025/02/18/xue-xi-zai-deepseek-r1-3b-shang-zuo-jian-dan-lora-wei-diao/"/>
      <url>/2025/02/18/xue-xi-zai-deepseek-r1-3b-shang-zuo-jian-dan-lora-wei-diao/</url>
      
        <content type="html"><![CDATA[<h2 id="LoRA-PEFT-for-DeepSeek-R1-Distill-Qwen-1-5B模型"><a href="#LoRA-PEFT-for-DeepSeek-R1-Distill-Qwen-1-5B模型" class="headerlink" title="LoRA PEFT for DeepSeek-R1-Distill-Qwen-1.5B模型"></a>LoRA PEFT for DeepSeek-R1-Distill-Qwen-1.5B模型</h2><blockquote><p>参考视频： <a href="https://www.bilibili.com/video/BV1pfKNe8E7F/?share_source=copy_web&vd_source=2ff04f0cc70237133af9bcd6be27a652">https://www.bilibili.com/video/BV1pfKNe8E7F/?share_source=copy_web&amp;vd_source=2ff04f0cc70237133af9bcd6be27a652</a></p></blockquote><hr><h3 id="模型准备"><a href="#模型准备" class="headerlink" title="模型准备"></a>模型准备</h3><p>首先到 hugging face 或者魔搭上下载相对应的模型文件。国内服务器用在 hugging face 上下载模型实在慢，我一般习惯用 git lfs 下载，所以就直接上魔搭了。模型文件可以在这里找到：</p><p>（说起来之前还用过 hugging face 的国内镜像网站，但那个很多时候也不稳定，所以一般现在都是直接先从魔搭上提前下好模型到本地，然后路径访问了,,,）</p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/1739957817449.jpg?imageSlim"/><h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><p>我这里是先拿了一个简单的数据集示例练手，具体可以参考夸克网盘：<a href="https://pan.quark.cn/s/a220f415b35c">https://pan.quark.cn/s/a220f415b35c</a></p><p>一般训练、微调都是用 json 格式，因此要先进行转换：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"./dataset/dataset.jsonl"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token keyword">for</span> s <span class="token keyword">in</span> samples<span class="token punctuation">:</span>        json_line <span class="token operator">=</span> json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span>s<span class="token punctuation">,</span> ensure_ascii<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>json_line <span class="token operator">+</span> <span class="token string">"\n"</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"data prepare done"</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"json"</span><span class="token punctuation">,</span> data_files<span class="token operator">=</span><span class="token string">"./dataset/dataset.jsonl"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>train_test_split <span class="token operator">=</span> dataset<span class="token punctuation">.</span>train_test_split<span class="token punctuation">(</span>test_size<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>train_dataset <span class="token operator">=</span> train_test_split<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span>test_dataset <span class="token operator">=</span> train_test_split<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span></code></pre><p>这里把脚本里的数据集按 prompt + completion 格式将每个元素转换为 JSON 字符串并写入文件，每行都是一个独立的 JSON 对象。例如：</p><p>{“prompt”: “Question 1: What is the first step to improving your singing voice?”, “completion”: “Answer 1: Begin by warming up your vocal cords with gentle exercises like humming or lip trills.”}</p><p>一般从 hugging face 上下载下来的数据集有不少也是 json 格式，比如之前用到的 ARC 数据集。但大部分都是直接打包成 arrow 格式，这时候直接用 datasets 库的 load_dataset() 方法加载数据集并分词就可以了。train_test_split() 方法可以按比例将数据分割成训练集和测试集。</p><h3 id="分词处理"><a href="#分词处理" class="headerlink" title="分词处理"></a>分词处理</h3><p>要让数据能够被模型识别，需要使用分词器将数据字符转换为真正的数字形式。一般模型都会带有 tokenizer，可以先加载：</p><pre class=" language-python"><code class="language-python">model_name <span class="token operator">=</span> <span class="token string">"/d2/mxy/Models/DeepSeek-R1-Distill-Qwen-1.5B"</span>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># model = AutoModelForCausalLM.from_pretrained(model_name)</span><span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span>    texts <span class="token operator">=</span> <span class="token punctuation">[</span>f<span class="token string">"&amp;#123;prompt&amp;#125;\n&amp;#123;completion&amp;#125;"</span> <span class="token keyword">for</span> prompt<span class="token punctuation">,</span> completion <span class="token keyword">in</span> zip<span class="token punctuation">(</span>examples<span class="token punctuation">[</span><span class="token string">"prompt"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> examples<span class="token punctuation">[</span><span class="token string">"completion"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>texts<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span>    tokens<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span> <span class="token operator">=</span> tokens<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> tokenstokenized_train_dataset <span class="token operator">=</span> train_dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>tokenized_test_dataset <span class="token operator">=</span> test_dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><p>需要定义分词处理函数并应用到加载后的数据集上。tokenize_function() 对每一条数据进行分词。</p><p><strong>不同的任务可能需要不同的分词处理。</strong>在这里我们的训练目标是一个生成任务，也就是模型根据当前 token 预测下一个 token，因此我们的 token[‘labels’] 定义为 token[‘input_ids’] 本身。假如是分类或问答任务，那么我们的 token[‘labels’] 就应当被定义为分类或回答的答案。</p><p>需要注意的是，tokenizer 原生支持的几个字段有以下五个：<strong>input_ids、attention_mask、label、type_ids、label_ids</strong>，这几个字段能够在 Trainer 里直接参与模型的训练，input_ids 即为输入，label 即为训练模型的输出。假如想要自定义其它字段的话，需要重写 Trainer 方法，使 Trainer 适配新的分词字段。</p><h3 id="加载训练配置并开始训练"><a href="#加载训练配置并开始训练" class="headerlink" title="加载训练配置并开始训练"></a>加载训练配置并开始训练</h3><p>transformers、peft 库自带了很多配置方法，直接调用即可。这里采用 8bits 量化，然后加载了一个 lora 模块用于微调模型。</p><pre class=" language-python"><code class="language-python">bnb_config <span class="token operator">=</span> BitsAndBytesConfig<span class="token punctuation">(</span>load_in_8bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>    model_name<span class="token punctuation">,</span>    quantization_config<span class="token operator">=</span>bnb_config<span class="token punctuation">,</span>    device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span><span class="token punctuation">)</span>lora_config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>    r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>    lora_alpha<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>    lora_dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>    task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">,</span><span class="token punctuation">)</span>model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> lora_config<span class="token punctuation">)</span>model<span class="token punctuation">.</span>print_trainable_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>    output_dir<span class="token operator">=</span><span class="token string">"./results"</span><span class="token punctuation">,</span>    eval_strategy<span class="token operator">=</span><span class="token string">"steps"</span><span class="token punctuation">,</span>    eval_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>    learning_rate<span class="token operator">=</span><span class="token number">2e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span>    per_device_train_batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>    fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    logging_dir<span class="token operator">=</span><span class="token string">"./logs"</span><span class="token punctuation">,</span>    logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>    num_train_epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>    weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>    save_strategy<span class="token operator">=</span><span class="token string">"steps"</span><span class="token punctuation">,</span>    save_steps<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>    save_total_limit<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>    run_name<span class="token operator">=</span><span class="token string">"deepseek-r1-distill-qwen-1.5b-lora"</span><span class="token punctuation">,</span><span class="token punctuation">)</span>trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>    model<span class="token operator">=</span>model<span class="token punctuation">,</span>    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>    train_dataset<span class="token operator">=</span>tokenized_train_dataset<span class="token punctuation">,</span>    eval_dataset<span class="token operator">=</span>tokenized_test_dataset<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Training..."</span><span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Saving model..."</span><span class="token punctuation">)</span>save_path <span class="token operator">=</span> <span class="token string">"./saved_models"</span>model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span>save_path<span class="token punctuation">)</span>tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span>save_path<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 合并LoRA权重到基础模型</span><span class="token keyword">from</span> peft <span class="token keyword">import</span> PeftModelfinal_save_path <span class="token operator">=</span> <span class="token string">"./final_saved_models"</span>base_model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>model <span class="token operator">=</span> PeftModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>base_model<span class="token punctuation">,</span> save_path<span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>merge_and_unload<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span>final_save_path<span class="token punctuation">)</span>tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span>final_save_path<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Done!"</span><span class="token punctuation">)</span></code></pre><h3 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h3><p>保存好 lora 权重的模型之后，就可以直接加载模型并用 transformers 自带的推理管线进行推理了。</p><pre class=" language-python"><code class="language-python">model_name <span class="token operator">=</span> <span class="token string">"./final_saved_models"</span>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipelinetext_generator <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span> model<span class="token operator">=</span>model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> num_return_sequences<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>prompt <span class="token operator">=</span> <span class="token string">"hello! tell me who are you?"</span>outputs <span class="token operator">=</span> text_generator<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"输出结构："</span><span class="token punctuation">,</span> outputs<span class="token punctuation">)</span>generated_text <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"generated_text"</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"生成的文本："</span><span class="token punctuation">,</span> generated_text<span class="token punctuation">)</span></code></pre><p>生成的结果如下：</p><pre><code>(GraphMoE) (/d1/.conda/vllm) xiangchao@h800-5-6gpu:/d2/mxy/LLM-PEFT/lora_peft$ python3 inference.py Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 13.76it/s]Device set to use cuda:0输出结构： [&#123;&#39;generated_text&#39;: &quot;hello! tell me who are you? i want to know more about you and your life.\nAlright, so I just wanted to find out who I am. I mean, I have a pretty decent life, but I don&#39;t really know much about who I am. I&#39;m not really sure where to start. Maybe I should look into my past experiences or what I&#39;ve been through. But I&#39;m not sure if that will help me figure out who I am. I mean, I know a lot about my own achievements, but I&quot;&#125;]生成的文本： hello! tell me who are you? i want to know more about you and your life.Alright, so I just wanted to find out who I am. I mean, I have a pretty decent life, but I don&#39;t really know much about who I am. I&#39;m not really sure where to start. Maybe I should look into my past experiences or what I&#39;ve been through. But I&#39;m not sure if that will help me figure out who I am. I mean, I know a lot about my own achievements, but I</code></pre><p>看起来像是 r1 模型的推理过程，限制了 max_tokens&#x3D;100，所以后面的输出被截断了。总之这样就算是完成啦！</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】DeepSeekMoE：我们需要更多Experts！</title>
      <link href="/2025/02/05/xue-xi-deepseekmoe/"/>
      <url>/2025/02/05/xue-xi-deepseekmoe/</url>
      
        <content type="html"><![CDATA[<h2 id="【笔记】DeepSeekMoE：迈向终极专家专业化"><a href="#【笔记】DeepSeekMoE：迈向终极专家专业化" class="headerlink" title="【笔记】DeepSeekMoE：迈向终极专家专业化"></a>【笔记】DeepSeekMoE：迈向终极专家专业化</h2><p>最近 DeepSeek 火爆整个2025年新春，趁着这波热度，想起来几个月前在 ACL 2024 上看过他们的论文，于是翻回来重新研究了一下。其实整篇论文的动机和故事都很直觉，但是不得不说人家的论文写得真的漂亮呀！</p><ul><li><strong>专家不够分化：</strong>以往的 MoE 模型一般也就是6个8个左右的 Expert 数量，但是任务量实际上要远远大于专家数，那么专家其实是不够“专”的。所以尽可能地把专家数拉高，然后寄希望于更多的专家分化知识能力。</li><li><strong>专家冗余：</strong>假设每个 Token 都需要常识知识，那么不管选哪个专家，这个专家都会保留常识知识能力，因而专家的“专业性”受到了影响，存在知识冗余，就浪费掉很多参数了。因此增设共享专家，所有 Token 来的时候都会选这个专家。</li></ul><hr><h4 id="模型架构："><a href="#模型架构：" class="headerlink" title="模型架构："></a>模型架构：</h4><p>所以方法也很直观了，论文里的 <strong>pipeline</strong> 也画得非常清楚：</p><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/1738763285958.jpg?imageSlim" alt="image-20240519223632525"></p><ol><li><p><strong>专家数量增加实现</strong>：</p><ul><li>具体来说，是通过拆分专家——比如把一个拆成两个——来增加专家数量。</li><li>主流的专家实际上就是个 FFN，FFN 实际上就是两个矩阵。假设这两个矩阵的大小分别是：$dh$ 和 $hd$ 。如果要把这个专家拆分成两个专家，实际上就是把矩阵拆成两个 $d*(h&#x2F;2)$ 和 $(h&#x2F;2)*d$ 这样。</li><li>假如原本一个 token 选择 top-1 专家来处理，拆成两个后，就变成选择 top-2 专家来出来。这样计算量和参数量是没有变化的。</li></ul><blockquote><p>实际上门控的计算量还是有变化的，路由模块需要计算的专家数增多了，自然计算量会大一些。</p></blockquote></li><li><p><strong>共享专家增设</strong>：</p><ul><li>有的专家是必选的，除此以外，每个 token 按照自己的喜好，再来选择 top-k。比如有 64 个专家，那么第一个专家是所有 token 都要选的，除此以外，每个 token 还从剩下的 63 个里选择自己的 top-1 专家。</li></ul></li></ol><p><strong>但这样就有一个新的考虑：Expert 数增大的情况下，是不是对于 Router 的能力依赖会更强呢？因为直观上来讲，每个专家更加分化，肯定也希望我的 Router 能够选得更精确，才能最大程度发挥专家的知识能力。所以是不是能用一个更 Stronger 的路由模块去进行一个进一步的优化？</strong></p><hr><h4 id="Experiment："><a href="#Experiment：" class="headerlink" title="Experiment："></a>Experiment：</h4><blockquote><p>不得不说人家的 Experiment 是真的充分，而且特别详细……</p></blockquote><p><strong>验证实验（2B参数模型）：</strong>验证DeepSeekMoE架构在中小规模下的有效性。对比了Dense模型、Hash Layer、Switch Transformer、GShard等MoE架构，</p><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/1738764304023.jpg?imageSlim" alt="image-20240519225817161"></p><p><strong>消融实验：</strong>主要验证了细粒度分割和共享专家的作用体现。</p><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/1738764486549.jpg?imageSlim" alt="image-20240519230406663"></p><p>当然论文还做了很多其它实验，在16B模型上的扩展实验、对齐实验（监督微调）、超大规模模型实验……这里就不一一列举了，感兴趣可以去读下原文~</p><p>这里突然想起来 ACL 2024 上还有一篇 Dynamic MoE，直观上感觉那篇的动态路由用到 DeepSeek 这样多专家的架构上提升应该更明显。因为其实 Top-K 这样固定选专家数量的策略，在专家数更多组合更多样的情况下，打分之后可能会有更大的差异性，所以按阈值来路由兴许有更好的效果。</p><hr>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】Patch-level Routing in MOE：针对局部图像区域分配专家，提高样本效率</title>
      <link href="/2024/05/28/xue-xi-patch-level-routing-in-moe/"/>
      <url>/2024/05/28/xue-xi-patch-level-routing-in-moe/</url>
      
        <content type="html"><![CDATA[<h2 id="【笔记】Patch-level-Routing：针对局部图像区域分配专家，提高样本效率"><a href="#【笔记】Patch-level-Routing：针对局部图像区域分配专家，提高样本效率" class="headerlink" title="【笔记】Patch-level Routing：针对局部图像区域分配专家，提高样本效率"></a>【笔记】Patch-level Routing：针对局部图像区域分配专家，提高样本效率</h2><ul><li><strong>现代神经网络模型（CNNs）：</strong>在处理大规模数据集时表现出色，但训练这些模型所需的数据和计算资源非常庞大。</li><li><strong>MOE混合专家模型：</strong>通过将输入数据动态分配给一组专家（子模型）中的一个或多个，能够提高计算效率。</li><li><strong>解决思路：</strong>将输入图像分割成若干个patch，每个patch独立地分配给不同的专家。针对图像数据的局部特征进行专家分配，以提高样本效率。</li><li><strong>突出问题：</strong>pMoE证明了为减少所需数量的训练样本，多批次能够提升效果与效率，但其理论来源与实验支撑还不够充分。</li><li><strong>论文贡献：</strong>理论证明分析了pMOE的优势，实际进行实验验证，详细量化其计算复杂度、内存使用与训练时间等方面的优点。</li></ul><hr><h3 id="模型架构："><a href="#模型架构：" class="headerlink" title="模型架构："></a>模型架构：</h3><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240529084952905.png?imageSlim"  width="500" /><h4 id="pMOE示例：卷积神经网络的专家模型"><a href="#pMOE示例：卷积神经网络的专家模型" class="headerlink" title="pMOE示例：卷积神经网络的专家模型"></a>pMOE示例：卷积神经网络的专家模型</h4><ol><li><p><strong>模型架构</strong></p><ul><li><p><strong>专家网络</strong>：pMOE 包含多个专家，每个专家是一个两层的卷积神经网络 (CNN)。每个专家负责处理输入样本的部分图像块。</p></li><li><p><strong>门控单元</strong>：每个专家对应一个可训练的路由门控，决定哪些图像块由哪个专家进行处理。</p></li></ul></li><li><p><strong>工作原理</strong></p><ul><li><strong>图像分块</strong>：先将输入图像（例如狐狸的图片）划分成若干个不重叠的图像块。</li><li><strong>路由选择</strong>：路由器根据图像块的特征值，生成一个评分分布，表示每个专家对每个图像块的适合程度。确保专家仅处理与其相关的图像块，过滤掉与分类无关的图像块。</li><li><strong>特征聚合：</strong>各个专家网络处理完图像块后，会生成相应的特征图。这些特征图会在模型的上层进行聚合，形成最终的特征表示。聚合后的特征用于最终的分类任务，图中展示了最终的分类结果为“狐狸 (Fox)”。</li></ul></li><li><p><strong>训练模式</strong></p><ul><li><strong>分离训练</strong>：首先训练路由器，然后固定路由器参数，训练专家网络。</li><li><strong>联合训练</strong>：同时训练路由器和专家网络，优化整体性能。</li></ul></li><li><p><strong>理论优势</strong></p><ul><li><strong>样本复杂度</strong>：pMOE 在达到相同泛化误差的情况下，所需的训练样本数量显著减少。本篇论文通过理论证明 pMOE 的样本复杂度和模型复杂度均较传统 CNN 模型降低。</li><li><strong>计算效率</strong>：在视觉任务中，pMOE 的训练和推理计算量分别比单一专家模型减少约 20% 和 50%，但仍保持相同的测试精度。</li></ul></li></ol><h4 id="论文贡献：三项主要研究成果"><a href="#论文贡献：三项主要研究成果" class="headerlink" title="论文贡献：三项主要研究成果"></a>论文贡献：三项主要研究成果</h4><ol><li><p>理论验证了只要路由门控向专家发送的补丁数 <code>l</code> 大于某个特定的阈值，pMOE便可降低模型复杂度与计算复杂度。</p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240529090936527.png?imageSlim"  width="500" /></li><li><p>能够有效提升模型的抗干扰能力，与将图像整体输入的MOE模型相比能够更好地丢弃类无关部分，处理分类任务。由于干扰减少，专家能够更高效地学习，直接导致样本复杂性和模型复杂性的降低。</p></li><li><p>通过实验验证了理论证明。</p></li></ol><h4 id="pMOE模型原理："><a href="#pMOE模型原理：" class="headerlink" title="pMOE模型原理："></a>pMOE模型原理：</h4><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240529092131288.png?imageSlim"  width="500" /><ol><li><strong>pMOE架构</strong>:<ul><li>pMOE模型包括 𝑘 个专家和相应的 𝑘  个路由器。</li><li>每个路由器为每个专家选择 𝑙 中的 𝑛 个patches（𝑙&lt;𝑛<em>l</em>&lt;<em>n</em>）。</li></ul></li><li><strong>路由门控</strong>：<ul><li>每个专家的路由器都包含一个可训练的门控向量 $w_{s}$ ∈ 𝑅𝑑。</li><li>给定一个样本 𝑥，路由器为每个patch 𝑗 计算一个路由值。</li></ul></li><li><strong>专家模型</strong>:<ul><li>每个专家是一个具有相同架构的两层卷积神经网络（CNN）。</li><li>设 𝑚 表示所有专家中的神经元总数。每个专家包含 𝑚&#x2F;𝑘 神经元。</li></ul></li><li><strong>模型定义</strong>:</li></ol><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240529092628373.png?imageSlim"  width="600" /><h4 id="训练方法："><a href="#训练方法：" class="headerlink" title="训练方法："></a>训练方法：</h4><ul><li>三种模型训练方法：</li></ul><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240529093413367.png?imageSlim"  width="500" /><hr><h3 id="思路拓展："><a href="#思路拓展：" class="headerlink" title="思路拓展："></a>思路拓展：</h3><ol><li><strong>文本片段分割分配专家：</strong>类似于图像的patch分割，可以将文本先分割成多个片段，如句子、段落或固定长度的n-grams。将这些文本片段分批独立地送入不同的门控单元，再分配给不同的专家，利用专家的多样性来处理不同类型的文本片段，是否能够提升计算效率和学习速度？</li><li><strong>层级结构专家分配：</strong>文本数据多数具有层级结构，如章节、段落、句子等。是否可以参照Patch-level Routing的方法，设计一种层级结构的专家分配策略，将标题、首段、尾段等分割出来再在不同层级上使用不同的专家？例如，顶层专家处理整体段落或章节，底层专家处理句子或词。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】MOELoRA：基于MOE应用LoRA微调</title>
      <link href="/2024/05/18/xue-xi-moelora/"/>
      <url>/2024/05/18/xue-xi-moelora/</url>
      
        <content type="html"><![CDATA[<h2 id="【笔记】MOELoRA-一种基于MOE多任务医疗应用下的高效参数微调方法"><a href="#【笔记】MOELoRA-一种基于MOE多任务医疗应用下的高效参数微调方法" class="headerlink" title="【笔记】MOELoRA: 一种基于MOE多任务医疗应用下的高效参数微调方法"></a>【笔记】MOELoRA: 一种基于MOE多任务医疗应用下的高效参数微调方法</h2><ul><li><strong>医疗领域的LLM微调困境：</strong>①任务多样性太大；②参数量大，微调成本太高</li><li><strong>解决思路：</strong>MOE与LoRA结合，针对特定领域任务进行微调</li><li><strong>突出问题：</strong>不同任务下的数据量在质量、规模上都存在差距（A任务有1000个数据，B任务有5000个数据，使得微调参数会倾向于样本量更大的任务，破坏少样本任务的处理性能。如何解决？）</li></ul><hr><h4 id="模型架构："><a href="#模型架构：" class="headerlink" title="模型架构："></a>模型架构：</h4><ul><li>多个专家作为可训练的参数，其中每个专家由一对低秩矩阵组成，以保持少量的可训练的参数。</li><li>为所有的MOELoRA层提出了一个任务动机的门函数，该函数可以调节每个专家的贡献，并为各种任务生成不同的参数。</li></ul><p><strong>使用MOELoRA对LLM的参数高效微调过程的可视化表示如下：</strong></p><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240519223632525.png?imageSlim" alt="image-20240519223632525"></p><ol><li><p><strong>Attention层</strong>：</p><ul><li>包含多头注意力机制的标准组件，其中Q（Query）、K（Key）、V（Value）各自通过LoRA适配器进行增强。</li><li>每个Q、K、V矩阵都通过MOE-LoRA增强，即每个矩阵都分别有专门的LoRA专家进行调整。</li><li>这些LoRA专家由底部的门控（Gate）单元动态选择，门控根据输入的任务ID决定激活哪些专家。</li></ul></li><li><p><strong>Layer Norm和Add层</strong>：</p><ul><li><p>Transformer架构中的每个子层输出后都会经过一个添加（Add）和层归一化（Layer Norm）操作。</p></li><li><p>在此架构中，W表示权重矩阵，包括由MoE-LoRA微调的参数。</p></li></ul></li><li><p><strong>Gate单元</strong>：</p><ul><li><p>根据输入的任务ID（如文档底部所示），决定哪些LoRA专家参与当前任务的计算。</p></li><li><p>确保每个特定任务都能调用最适合它的专家组合，从而实现高效的任务特定微调。</p></li></ul></li><li><p><strong>训练与固定权重</strong>：</p><ul><li>图中用火焰符号标注的权重（W）是可训练的，而用雪花符号标注的是固定的，表明在微调过程中这部分权重不会被更新。</li></ul></li></ol><hr><h4 id="MOELoRA层："><a href="#MOELoRA层：" class="headerlink" title="MOELoRA层："></a>MOELoRA层：</h4><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240519225817161.png?imageSlim" alt="image-20240519225817161"></p><ul><li><p>每个专家都被替换为了一个低秩矩阵相乘的形式</p></li><li><p>任务 $T_{j}$ 样本的线性层与MOELoRA层配对的前向过程表示为：</p></li></ul><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240519230406663.png?imageSlim" alt="image-20240519230406663"></p><hr><h4 id="任务驱动的门控单元："><a href="#任务驱动的门控单元：" class="headerlink" title="任务驱动的门控单元："></a>任务驱动的门控单元：</h4><p>权重计算公式：</p><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240519231110060.png?imageSlim" alt="image-20240519231110060"></p><ul><li>任务驱动的门函数为每个任务产生一组<strong>独特</strong>的贡献权值</li></ul><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240519231241393.png?imageSlim" alt="image-20240519231241393"></p><p>如果门函数是由输入向量x驱动的，权向量将在样本之间有所不同。因此只将任务身份输入到门函数中，这意味着每个样本都将拥有其独特的ωj，从而导致一个特定于样本的微调参数矩阵。<em>（这个地方有点不太理解……？）</em></p><ol><li><strong>任务定制：</strong>每个任务都使用一组参数进行微调，这有助于学习更多特定于任务的信息，缓解数据不平衡的问题。</li><li><strong>推理效率：</strong>检索到的微调LLM显示出减少的推理延迟。这是由于消除了与LoRA层相关的额外正向计算的需要。</li></ol><hr><h4 id="算法实现过程："><a href="#算法实现过程：" class="headerlink" title="算法实现过程："></a>算法实现过程：</h4><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/image-20240519231739537.png?imageSlim" alt="image-20240519231739537"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】MOE混合专家模型</title>
      <link href="/2024/05/08/xue-xi-moe-hun-he-zhuan-jia-mo-xing/"/>
      <url>/2024/05/08/xue-xi-moe-hun-he-zhuan-jia-mo-xing/</url>
      
        <content type="html"><![CDATA[<h2 id="【笔记】MOE混合专家模型"><a href="#【笔记】MOE混合专家模型" class="headerlink" title="【笔记】MOE混合专家模型"></a>【笔记】MOE混合专家模型</h2><ul><li>与稠密模型相比， <strong>预训练速度更快</strong></li><li>与具有相同参数数量的模型相比，具有更快的 <strong>推理速度</strong></li><li>需要 <strong>大量显存</strong>，因为所有专家系统都需要加载到内存中</li><li>在 <strong>微调方面存在诸多挑战</strong>，但对混合专家模型进行 <strong>指令调优具有很大的潜力</strong>。</li></ul><hr><h4 id="模型架构："><a href="#模型架构：" class="headerlink" title="模型架构："></a>模型架构：</h4><p>MOE是一种基于Transformer架构的模型，主要由两个部分组成：</p><ul><li><strong>稀疏 MoE 层</strong>: 取代传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家模型”，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。</li><li><strong>门控网络单元</strong>: 决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，“More”这个 <code>token</code> 可能被发送到第二个专家，而“Parameters”这个 <code>token</code> 被发送到第一个专家。有时，一个 <code>token</code> 甚至可以被发送到多个专家。门控单元的设计是 MoE 使用中的一个关键点，与网络的其他部分一同进行预训练。</li></ul><p><img src="C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20240508171027413.png" alt="image-20240508171027413"></p><p>其中，门控网络通常是一个带有Softmax的简单网络，学习将输入发送给哪一个专家的分类。</p><p><img src="C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20240508171640190.png" alt="image-20240508171640190"></p><p>所有专家模型都会对所有输入进行运算，但通过门控网络的输出进行加权乘法操作。</p><p><img src="C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20240508171722067.png" alt="image-20240508171722067"></p><h4 id="数据需求："><a href="#数据需求：" class="headerlink" title="数据需求："></a>数据需求：</h4><p>对于MOE模型，数据的质量比数据数量更重要。多个专家模型的训练需要确保有涵盖每一个领域的大量数据作为支撑，专家 gating network 也需要分配平衡。故而常需要一个分类器质检，确保数据的质量。</p><p>MOE的更多参数也意味着我们需要更多数据去训练。</p><h4 id="并行训练："><a href="#并行训练：" class="headerlink" title="并行训练："></a>并行训练：</h4><p>MOE在训练中可以采用数据并行或模型并行的训练方法。</p><p><strong>· 数据并行：</strong>多个显卡对同一个MOE（参数一致），每个显卡进行不同的专家模型训练。将数据并行地发送到MOE之中，实现数据并行训练。</p><p><strong>· 模型并行：</strong>将MOE的多个专家拆到多个显卡当中，依次输入相关参数，但每次可能会索引到其它卡的专家模型之中，故而需要卡间通信。</p><p><img src="C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20240508172345121.png" alt="image-20240508172345121"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】LoRA微调原理</title>
      <link href="/2024/05/07/xue-xi-lora-wei-diao-yuan-li/"/>
      <url>/2024/05/07/xue-xi-lora-wei-diao-yuan-li/</url>
      
        <content type="html"><![CDATA[<h2 id="【笔记】Lora微调原理"><a href="#【笔记】Lora微调原理" class="headerlink" title="【笔记】Lora微调原理"></a>【笔记】Lora微调原理</h2><blockquote><p>大语言模型微调算法</p></blockquote><hr><p><strong>基本原理：</strong>低秩矩阵相乘升维，与预训练权重矩阵叠加，实现模型微调</p><p><strong>原理简述：</strong>冻结预训练的模型参数，然后在Transfomer的每一层中加入一个可训练的旁路矩阵（低秩可分离矩阵），接着将旁路输出与初始路径输出相加输入到网络当中。在训练的时候，只训练这些新增的低秩矩阵的参数。其中低秩矩阵由两个矩阵组成，第一个矩阵负责降维，第二个矩阵负责升维，中间层维度为r，两个低秩矩阵能够大幅度减小参数量。</p><p><img src="C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20240508103941588.png" alt="image-20240508103941588"></p><p><strong>原理解释：</strong></p><p>输入为<code>x</code>，输出为<code>h</code>，预训练模型：</p><p>$h&#x3D;W_{0}x$</p><p>在全参数量上微调（regular finetuning）：</p><p>$h&#x3D;W_{0}x+△Wx$</p><p>如果采用低秩可分离矩阵（LoRA），则：<br>$$h&#x3D;W_{0}x+△Wx&#x3D;W_{0}x+BAx $$</p><p>在这个过程中，最核心的步骤便是将 $△W$ 进行分解，将其分解为两个低秩矩阵。如果只使用单独的一个矩阵，那么参数量为 $$d<em>d$$，如果使用低秩可分离矩阵，此时原来的大矩阵可分解为两个秩为 $r$ 的大小为 $$d</em>r$$ 的小矩阵，参数量大小为 ，由于 $$r&lt;d$$ 所以可以大幅度减少参数量。</p><p>即：*<em>A矩阵采用一个 $$d</em>r$$ 矩阵，B矩阵采用一个 $$r<em>d$$ 矩阵，总参数量 $$2r</em>d$$ **</p><p><strong>公式推导：</strong></p><p>LoRA 作用形式：$W&#x3D;W_{plm}+aW_{lora}&#x3D;W_{plm}+aB_{lora_{zeros}}*A_{lora_{gaussian}}$</p><p>论文中，将 LoRA 作用在 Query 与 Value 权重矩阵上效果最好：</p><p><code>attention query</code> 映射矩阵：$W_{q}&#x3D;W_{q_{plm}}+aW_{q_{lora}}$</p><p><code>attention value</code> 映射矩阵：$W_{v}&#x3D;W_{v_{plm}}+aW_{v_{lora}}$</p><p><img src="C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20240508141215200.png" alt="image-20240508141215200"></p><p><code>Attention</code> 计算结果：</p><p>$head&#x3D;softmax(Q,K)XW&#x3D;softmax(Q,K)X(W_{v_{plm}}+aX<em>W_{v_{lora}})&#x3D;softmax(Q,K)XW_{v_{plm}}+a</em>softmax(Q,K)XW_{v_{lora}}$</p><hr><p><strong>低秩分解：</strong></p><p>· 如何实现低秩分解？</p><p>低秩分解是一种将矩阵分解为低秩近似的技术。在LORA中，我们使用低秩矩阵来编码参数增量。假设我们有一个预训练的权重矩阵，它的维度是高的。我们可以将<strong>这个矩阵分解为两个低秩矩阵的乘积</strong>，例如：$$W&#x3D; UV^T$$其中U 和V是低秩矩阵。</p><pre class=" language-text"><code class="language-text">##矩阵中，row2 = row1 * 2，row3 = row1*3，也就是说，矩阵中的每一行，都可以通过第一行线性表示。A = [[1, 2, 3],[2, 4, 6],[3, 6, 9]]#任意一行，总可以用其他两行的线性组合来表示。B = [[1, 2, 3],   [7, 11, 5],[8, 13, 8]]#任意一行，都不能从其余行的线性组合中推导而来。C = [[1, 0, 0],[0, 1, 0],[0, 0, 1]]#求解秩：A = np.array(A)B = np.array(B)C = np.array(C) print("Rank of A:", np.linalg.matrix_rank(A)) # 1print("Rank of B:", np.linalg.matrix_rank(B)) # 2print("Rank of C:", np.linalg.matrix_rank(C)) # 3# 对矩阵A来说，由于只要掌握其中的任意一行，其余行都可以由这一行线性推导而来，因此A的秩是1。# 对矩阵B来说，由于只要掌握其中的任意两行，其余行都可以由这两行线性组合推导而来，因此B的秩是2。# 对矩阵C来说，由于必须完全掌握三行，才能得到完整的C，因此C的秩是3。# 简单理解一下，秩表示的是矩阵的信息量。#如果矩阵中的某一维，总可以通过其余维度线性推导而来，那么对模型来说，#这一维的信息是冗余的，是重复表达的。#对A和B的情况，我们称为秩亏（rank deficient），#对C的情况，我们称为满秩（full rank）。</code></pre><p>· 对A矩阵，采用随机高斯初始化；对B矩阵，采用zero初始化。这样做的原因是要<strong>保证梯度更新有效。</strong></p><p><img src="C:\Users\86133\AppData\Roaming\Typora\typora-user-images\image-20240508142401355.png" alt="image-20240508142401355"></p><p>$ℎ_{𝑖}^{(2)}$ 是输出向量 $h$ 的第 $i$ 个元素，$B_{i,k}$ 是矩阵 $B$ 的元素，$A_{i,k}$ 是矩阵 $A$ 的元素，而 $x_{j}$ 是输入向量 $x$ 的第 $j$ 个元素。</p><p>计算权重 $B$ 中元素 $B_{i,k}$ 的偏导数来进行梯度下降。固定 $i$ 和 $k$，对所有的 $j$ 进行求和，将 $A$ 的对应元素与输入向量的元素相乘并累加。</p><p>$B_{i,k}$ 的每个更新都是由 $x$ 和 $A$ 的对应元素的乘积影响的，故而对A矩阵，采用随机高斯初始化；对B矩阵，采用zero初始化。</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【手记】水手与海</title>
      <link href="/2024/04/19/shou-ji-shui-shou-yu-hai/"/>
      <url>/2024/04/19/shou-ji-shui-shou-yu-hai/</url>
      
        <content type="html"><![CDATA[<h1 id="【手记】水手与海"><a href="#【手记】水手与海" class="headerlink" title="【手记】水手与海"></a>【手记】水手与海</h1><hr><p>从狼奔豕突和闪闪发光的漫游者之间变换的林中猎手的梦中醒来，迎面是咸腥的海风和刚刚降下风帆的帆船。</p><p>在船只与陆地之间有零星的水手正在搬运货物，古铜色虬扎的皮肤诉说着海上曾经的波涛惊骇。</p><p>你伫立在原地，水手打量着你，远比一件货品来得匆匆。</p><p>你上前礼貌地询问方位，时间几何，来处归途，你描述模糊记忆里的那不勒斯，希冀从他人口中获得吉光片羽真实的引线。</p><p>有些人熟视无睹，有的水手向你摇头拒绝交谈，有的人留下只言片语，也有人停下来同你攀谈，你一一道谢。</p><p>你得到了一些讯息，此处远离内陆，少有人烟，与你所见并无二致。而停泊的帆船很快就要再度起航，返航前往尚算和平的王国城都。</p><p>在城郭高高的城墙被水泥钢筋乃至鲜血浇灌垒砌之前，恶龙突破重重防守，掠走王国的明珠，为王国上空覆下浓重的阴影。</p><p>——恶龙。那并不是一个人的梦魇，扇动着看不见的死亡之翼，伴随着诡秘的雾气，恶龙掀翻船只，向海底献上众人的遗骨。</p><p>水手止了声音，眼里蒙上一层海雾般的阴翳。</p><p>像想起征战的英雄，往日的故友，更甚是早逝的小女儿。</p><p>他最终调转了话头，询问你的来时去往。</p><hr><p>一番交谈后，你表示希望一同搭乘船只前往遥远的王国，记忆里的那不勒斯与水手口中描述的城郭并无二致。但摸遍浑身上下，却有些尴尬地发现自己身上并无分文。</p><p>你于是捧出了那颗宝石。</p><p>“你……”</p><p>水手看向你，起了个话头，最终却什么也没说。他应允了你上船的请求，却没有收下那枚宝石。</p><p>“你认识一个……紫色眼睛的旅行者吗？”</p><p>“不……没什么。”</p><p>在即将开口的时候，他却摇摇头，示意你不用再向他传达你的回答。</p><p>你默然。</p><p>自冲角看向向船尾高悬的船灯，主桅旁的舱室壁上搁靠着一人高细长的桨船和数量不菲的木桶。</p><p>当最后一缕残阳没入海平面，船长发号施令，舵手掌握方向，水手们扬起船帆，将锚拉回甲板上。</p><p>跟随者罗盘的指引，船只谨慎地沿着海岸线行驶。</p><hr><p>接连几天都是好天气，海面上风平浪静。</p><p>这段时日来，你一直在船上做一些力所能及的事，譬如搬抬重物，值夜，更甚是哄小孩儿入睡。</p><p>当夜幕降下的时候，小孩子纷纷被传唤进船舱内。“勇士”收起了长剑，自己掖好了被角。“恶龙”也归了巢，久久不肯入眠，拉着大人的衣角请求一个甜蜜的睡前故事。</p><p>船上讲述的睡前故事虽夜夜相似，但如同摇篮上的床铃，会在梦境里变幻着绮丽的花样。</p><p>“故事的一开始是毁坏一切的邪恶巨龙，灾难的降临，流离失所的人们，弥漫的硝烟和灼热的泪水。”</p><p>“那我也会和爸爸妈妈分开吗？”</p><p>重复的故事也会衍化出千百个为什么，而当干净得只有渴求着答案的目光望向你时，第一时间，你难以拒绝。</p><p>有时候孩子会像这样小心翼翼地问。</p><p>“一定不会分开。”</p><p>“那我会和克鲁鲁分开吗？”</p><p>孩子的眼睫颤动着，小声地再度确认。</p><p>你没有再做出回应。</p><p>孩子的母亲向你传达的故事的终局，在孩子的枕席边，睡梦之前——溪流总会重新流动，鲜花将于暖春盛放，思乡者终会重归故土。</p><p>勇者历经重重磨难，必然战胜恶龙，救回公主。</p><p>你摸了摸孩子的头，没有多做解释，只是慢慢将希望倾吐于唇齿之间。</p><p>“勇者踏上讨伐恶龙的征途，勇者帮助了形形色色的人，勇者历经了千辛万苦。”</p><p>“就勇者一个人吗？”</p><p>“他孤身一人。”</p><p>你顿了顿，像是在咀嚼故事中那位英雄的奋战史——</p><p>“勇者并不鲁莽行事，他虽然勇敢，却从未冲动。”</p><hr><p>夜色渐浓，舱室外越发安静，只有风声和潺潺的水声依旧。所有在甲板上工作的成年人逐渐变得无精打采，身体逐渐变得僵冷。而正在此刻，甲板上升起了低低的歌声。</p><p>歌声从四面透风的船长室传来，在甲板上盘旋，舵手用低沉的嗓音唱着一首小调。</p><p><del>在汪洋的怀抱深处，躺着我们的故乡。游荡的船只知晓她的存在，他们唱着歌儿离开这里，归来时亦在海滩上吟唱</del></p><p><del>她被一群平凡之人所深深爱着，死在她怀里的人们也依旧爱她。流浪旅人在汪洋徜徉，梦魂却在她的边界游荡，谁会忘记她？我们的故乡</del></p><p><del>在旅途中出生的孩子，未曾见过她的容颜，却仍追寻海岸线。不愿割舍的她携着海风，亦将新生儿爱抚</del></p><p>这并不是一首搏击浪潮的海洋之歌，而是一首莼鲈之思的缱倦乡曲。思念着远方厚重的泥土，无论我们身在何处，星辰之下抑或大海之上。</p><p>值夜的水手们默默挺起了脊背，先是一个人，接着两个，三个……一起轻轻哼唱。</p><p>歌声浮在甲板上，船室将它隔绝，此时母亲们的摇篮曲刚落下最后一个音节，有些未完的睡前故事也进行到最后一段。</p><p>“恶龙从巢穴中苏醒，转身向着勇者咆哮，声音震耳欲聋。”</p><p>“勇者面向恶龙，守卫着背后的家园，高举起象征着勇气与正义的勇者之剑。”</p><p>“勇者说道：”</p><p>“‘大陆的花朵将重新绽放，公主将回归王国，而你，恶龙，将死于我的剑下。’”</p><p>如同幼猫酣睡一般心满意足，孩子们安然落入英雄和鲜花的梦之旅中。</p><hr><p>怒号的狂风和热情的涌流陆续带走了“她”的一部分。</p><p>浪潮裹挟着一部分的“她”奔向远方，又有一部分的“她”被风所吹散。</p><p>海水捧着扬帆的船只，一部分的“她”不断下沉，沉到陈年船骸的怀里，船骸的金色桅杆斜斜地竖着，无数的珍宝在海底寂静无声。</p><p>时间流淌着，现世的人们从不回头，“她”漂浮于水面之上，在半空中徘徊，分散继而又聚拢，“她”无法仅凭借自己的力量鼓动风帆，游魂并不是真正的自由。</p><p>“她”是否在等待着什么？是还有什么未竟之事吗？以至于被束缚于此，以致于在漫长的轮回里徜徉，灵魂久久不灭。</p><p>——最后一部分的“她”，打着旋，拂过旅行者的发梢，追逐着一个个远望的目光，一张张模糊的面影。</p><p>但“她”总不能停驻太久，她只能借助风声和浪潮拍打的声音，祈祷般为所见之人祝福着，与那思乡的曲调相合。</p>]]></content>
      
      
      <categories>
          
          <category> 收录集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】NovelAI进行绘画创作</title>
      <link href="/2024/04/14/xue-xi-novelai-shi-yong-xin-de/"/>
      <url>/2024/04/14/xue-xi-novelai-shi-yong-xin-de/</url>
      
        <content type="html"><![CDATA[<h1 id="NovelAI进行绘画创作"><a href="#NovelAI进行绘画创作" class="headerlink" title="NovelAI进行绘画创作"></a>NovelAI进行绘画创作</h1><p>近期在尝试制作的Unity游戏项目需要一些CG素材，由于实在缺乏资金向画师约稿……所以这边就想着用NovelAI或者Midjourney这样的AI绘画工具来生成一些令人满意的素材~</p><p>试了一下，还是NovelAI的生成效果比较好。不过用起来也很麻烦呀，需要去调整很多东西，改一些参数，合理选择<code>Tag</code>才能保证产图的效果。</p><p>在这个过程中，也可以学到一些调参方面的经验吧，大概。</p><hr><blockquote><p><strong>登录已有的NovelAI网页资源</strong></p></blockquote><p>XianYun Web：<a href="https://nai3.xianyun.cool/main#/gen_img">https://nai3.xianyun.cool/main#/gen_img</a></p><p>不建议用NovelAI官网提供的版本……之前试着用过，效果并不如一些整合后的资源网所能提供的效果好。</p><h3 id="一、界面介绍"><a href="#一、界面介绍" class="headerlink" title="一、界面介绍"></a>一、界面介绍</h3><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/NovelAI%E7%95%8C%E9%9D%A2%E5%9B%BE.jpg?imageSlim"></p><p><strong>主绘图界面主要分为三部分：Prompts编辑、绘制次数设定、绘图参数调整。</strong></p><h4 id="Prompts编辑"><a href="#Prompts编辑" class="headerlink" title="Prompts编辑"></a>Prompts编辑</h4><p>在<code>Prompts</code>编辑界面，主要是<code>Tag</code>的选取。选取<code>Tag</code>对图像质量有着不可忽略的指导作用，一定要确保<code>Tag</code>符合自己的要求标准。</p><ol><li><code>Tag</code>必须是英文输入，以”,”为间隔。实际上输入语言文本也能够进行生成，不过不太建议这样做。（虽然不太清楚它的文本序列<code>Encoder-Decoder</code>怎么设置的，应该是从一串文本序列里识别到特定的标签然后拿标签进行指导生成？）可以用上面自带的内容翻译进行中英互译。</li><li><code>Tag</code>里的括号表示加权，对某一个提示词的指导权重进行加强，直白点就是围绕这个提示词的生成效果变得更重。</li><li>如果不知道该用哪些提示词比较好，可以在左侧的<code>Tag</code>编辑器里预选一些可参考的提示词。</li><li>可以在<code>Tag</code>里指导生成图像所选用的画师画风。格式为”artist: xxx”，只要是模型预训练过的画师数据都可以进行指导生成。</li><li>词条笔记本、上传词条、随机<code>Tag</code>这些，日常使用很少用到，不作详述。</li></ol><h4 id="绘图参数调整"><a href="#绘图参数调整" class="headerlink" title="绘图参数调整"></a>绘图参数调整</h4><p>在调参界面，可以设定模型绘制的一些参数。一般来说都不必刻意去进行更改。</p><p><strong>长宽设置：</strong><code>Width</code>和<code>Height</code>可以指定生成的图像大小比例，特定分辨率需要勾选启用分辨率才能应用生成。一般来说要增加图像的长宽像素的话，需要更多的显存。大尺度的图像一致性会随着分辨率的提高而变差（模型是在 512x512 的基础上训练的）。非常小的值（例如 256 像素）也会降低图像质量。</p><p><strong>采样器设置：</strong>目前好用的有 <code>Euler</code>，<code>Euler a</code>（更细腻），和 <code>DDIM</code>。推荐 <code>Euler a</code> 和 <code>DDIM</code>，**新手推荐使用 <code>Euler a</code>**。</p><p><code>Euler a</code> 富有创造力，不同步数可以生产出不同的图片。调太高步数 (&gt;30) 效果不会更好。</p><p><code>DDIM</code> 收敛快，但效率相对较低，因为需要很多 step 才能获得好的结果，<strong>适合在重绘时候使用</strong></p><p><code>LMS</code> 和 <code>PLMS</code> 是 <code>Euler</code> 的衍生，它们使用一种相关但稍有不同的方法（平均过去的几个步骤以提高准确性）。大概 30 step 可以得到稳定结果</p><p><code>PLMS</code> 是一种有效的 LMS（经典方法），可以更好地处理神经网络结构中的奇异性</p><p><code>DPM2</code> 是一种神奇的方法，它旨在改进 DDIM，减少步骤以获得良好的结果。它需要每一步运行两次去噪，它的速度大约是 DDIM 的两倍。但是如果你在进行调试提示词的实验，这个采样器效果不怎么样……</p><p><code>Euler</code> 是最简单的，因此也是最快的之一。（搬运自AiDraw的文档指导~）</p><p>※ <code>Steps</code>一般不要设置得太少，毕竟是基于Stable Diffusion降噪生成图像。但设置得太高也会徒增生成时间。一般来说拉到28就够了。</p><h4 id="绘制次数设定"><a href="#绘制次数设定" class="headerlink" title="绘制次数设定"></a>绘制次数设定</h4><p>简单易懂，设定一次绘制多少幅画。</p><h3 id="二、拓展功能"><a href="#二、拓展功能" class="headerlink" title="二、拓展功能"></a>二、拓展功能</h3><p>除了最重要的NovelAI绘图部分，网站还设定了许多额外的拓展功能可供使用。</p><h4 id="图像超分辨率"><a href="#图像超分辨率" class="headerlink" title="图像超分辨率"></a>图像超分辨率</h4><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/NovelAI-%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87.jpg?imageSlim"></p><p>如果对生成的图像清晰度要求比较高，可以在这里对图像进行清晰化缩放。</p><p>一般来说，选择<code>SwinIR_4x</code>放大算法就够用啦。</p><h4 id="Tag编辑器"><a href="#Tag编辑器" class="headerlink" title="Tag编辑器"></a>Tag编辑器</h4><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/NovelAI-Tag%E7%BC%96%E8%BE%91.jpg?imageSlim"></p><p>如果对绘画功能的提示词<code>Tag</code>设置不明晰的话，可以借助<code>Tag</code>编辑器辅助选择<code>Tag</code>。</p><p>预设提供的<code>Tag</code>标签有很多，随心所欲搭配直到生成令人满意的结果吧。</p><p>除此之外，还有<strong>图像反推Tag</strong>、<strong>图像Tag识别</strong>两项功能，从提供的图像中识别出对应的标签（可能是基于CLIP模型做的吧？之前读<code>Diffu Musk</code>论文的时候读到过类似功能的实现方法）。</p><hr><p>总的来说，差不多就是这样啦。之前看很多大佬通过NovelAI搭配出了非常好看的生成效果，现在自己也试着尝试一下吧。干什么都不容易哇，仅仅是利用AI进行一个生成，想要得到好的结果也需要很多学习和努力。</p><p>姑且自己设置提示词生成了一点素材，后续再挑拣一些后期修改放到Unity项目里吧：</p><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/2bdb3aa480d5d437ac1a44f83ddd795.png?imageSlim"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经验分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【学习】Linux服务器上升级Python版本</title>
      <link href="/2024/03/11/xue-xi-linux-fu-wu-qi-shang-sheng-ji-python-ban-ben/"/>
      <url>/2024/03/11/xue-xi-linux-fu-wu-qi-shang-sheng-ji-python-ban-ben/</url>
      
        <content type="html"><![CDATA[<h1 id="在Linux服务器上升级Python版本"><a href="#在Linux服务器上升级Python版本" class="headerlink" title="在Linux服务器上升级Python版本"></a>在Linux服务器上升级Python版本</h1><p>近段时间在用导师给的服务器跑项目，所以升级了一下 Python 版本。这里分享下自己在升级 Python 时的流程和遇到的一些问题~</p><p>参考博客：<a href="https://blog.csdn.net/Jxq_IT/article/details/103971862">https://blog.csdn.net/Jxq_IT/article/details/103971862</a></p><hr><blockquote><p><strong>直接更新系统Python版本</strong></p></blockquote><h4 id="一、查看系统中的Python版本"><a href="#一、查看系统中的Python版本" class="headerlink" title="一、查看系统中的Python版本"></a>一、查看系统中的Python版本</h4><p>我的服务器是 Ubuntu 18.04.6 版本，之前预装的 Python 版本是 3.7.13，由于论文需要，故要升级到 3.10 版本。首先我们用如下指令查看当前系统的 Python 版本：</p><ol><li><p>使用<code>Python</code>或<code>Python3</code>命令：</p><pre class=" language-text"><code class="language-text">python --versionpython3 --version</code></pre></li><li><p>直接查看<code>bin</code>文件夹下安装的 Python 目录：</p><pre class=" language-text"><code class="language-text">ls /usr/bin/python*</code></pre></li></ol><h4 id="二、利用wget下载对应版本的Python包"><a href="#二、利用wget下载对应版本的Python包" class="headerlink" title="二、利用wget下载对应版本的Python包"></a>二、利用wget下载对应版本的Python包</h4><ol><li><p>在当前的目录下，安装 Python 包。指令如下：</p><pre class=" language-text"><code class="language-text">wget https://www.python.org/ftp/python/3.10.13/Python-3.10.13.tgz</code></pre></li><li><p>上传服务器并解压缩：</p><pre class=" language-text"><code class="language-text">tar -xf Python-3.10.13.tgz</code></pre></li><li><p>进入解压缩后的文件夹，并编译：</p><pre class=" language-text"><code class="language-text">./configure && make && make install</code></pre></li></ol><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%BE_1.jpg?imageSlim" alt="Python官网的相关版本"></p><p>网址：<a href="https://www.python.org/downloads/source/">https://www.python.org/downloads/source/</a></p><p><strong>编译结束后，正常情况下会装到&#x2F;usr&#x2F;local&#x2F;bin目录下，记得去看一下！</strong></p><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%BE_2.jpg?imageSlim"></p><h4 id="三、创建软连接"><a href="#三、创建软连接" class="headerlink" title="三、创建软连接"></a>三、创建软连接</h4><ol><li><p>首先检查系统是否已经有 <code>python3、python </code>的软链接，可以使用以下命令：</p><pre class=" language-text"><code class="language-text">ls -l /usr/bin/python3ls -l /usr/bin/python</code></pre></li><li><p>删除现有软连接，并创建新的软连接指向<code>Python 3.10</code>：</p><pre class=" language-text"><code class="language-text">sudo rm /usr/bin/python3sudo rm /usr/bin/pythonsudo ln -s /usr/local/bin/python3.10 /usr/bin/python3sudo ln -s /usr/local/bin/python3.10 /usr/bin/pythonsudo ln -sf /usr/local/bin/python3.10-config /usr/bin/python-config</code></pre></li></ol><p>​※注意，如果已经是root用户的话，不需要加sudo哦</p><h4 id="四、检查环境变量"><a href="#四、检查环境变量" class="headerlink" title="四、检查环境变量"></a>四、检查环境变量</h4><p>要运行一下 <code>echo $PATH</code> 来查看当前的 <code>PATH</code> 设置，确保 <code>/usr/bin</code>（或存放了 Python 3.10 软链接的目录）在任何可能包含旧 Python 版本的目录之前。否则的话，执行<code>python --version</code>仍然会显示之前的版本！</p><pre class=" language-text"><code class="language-text">export PATH=/usr/bin:$PATH</code></pre><p>执行这个命令，暂时将将 <code>/usr/bin</code> 移到 <code>PATH</code> 的前面。</p><p>最后，再运行一遍<code>python --version</code>来检查。如果还是不行，试试用<code>hash -r</code>清除一下bash的缓存，或重新打开 shell 会话。</p><blockquote><p><strong>使用Conda管理Python版本</strong></p></blockquote><p>使用Conda来进行管理的话，操作起来就方便多啦。当然你需要先保证系统里预装了Conda。</p><pre class=" language-text"><code class="language-text">conda create -n myenv python=3.10conda activate myenv</code></pre><p>这样会创建一个名为 <code>myenv</code> 的新环境，其中安装了 <code>Python 3.10</code>，然后激活这个环境。</p><p>如果 <code>conda activate</code> 命令提示不能正确使用，按它的提示来，初始化一下 Conda 对 shell 的支持：</p><pre class=" language-text"><code class="language-text">conda init bash</code></pre><hr><p>好的，以上就是我的解决过程啦！希望能够帮到你~⁽(◍˃̵͈̑ᴗ˂̵͈̑)⁽</p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经验分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【手记】2024-新春寄语</title>
      <link href="/2024/02/10/shou-ji-2024-xin-chun-ji-yu/"/>
      <url>/2024/02/10/shou-ji-2024-xin-chun-ji-yu/</url>
      
        <content type="html"><![CDATA[<h1 id="【手记】2024-新春寄语"><a href="#【手记】2024-新春寄语" class="headerlink" title="【手记】2024-新春寄语"></a>【手记】2024-新春寄语</h1><hr><p>xxx</p><p>现在的烟花款式变得越来越多啦，造型和花型也都越来越漂亮，有点变相地羡慕小侄子们嘞。</p><p>小时候总在期待着的放烟花的活动，到现在这个年纪，早已不执着于那份纯真的趣味，仅仅只会欣赏烟火绽放瞬间夺目的光点。点燃它们的乐趣就给侄子们享受啦。</p><p>曾经的快乐总会伴着时光慢慢地在心里改变。随着自身的成长，每年的长度也都不尽相同。或许十年之后，一年之期和现在也不会再是同样的感受。</p><p>所以，好好把握当下叭！多陪伴身边的人，用双手掬起流淌在掌心的岁月。人生漫漫，每一年都无比珍重，每一刻都独特耀眼。<br>新年快乐！</p><p>愿新的一年里，也能怀揣这份小小的幸福，度过每一日的一点一滴。</p><p>xxx</p><p><img src="https://cyan-1314223569.cos.ap-beijing.myqcloud.com/test/2024.jpg?imageSlim"></p>]]></content>
      
      
      <categories>
          
          <category> 感悟 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 寄语 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【手记】小径</title>
      <link href="/2023/10/26/shou-ji-xiao-jing/"/>
      <url>/2023/10/26/shou-ji-xiao-jing/</url>
      
        <content type="html"><![CDATA[<h1 id="【手记】小径"><a href="#【手记】小径" class="headerlink" title="【手记】小径"></a>【手记】小径</h1><blockquote><p>トレイル（小径）——cover.mamenoi</p></blockquote><hr><p>xxx</p><p>掩埋在纯白细沙里的脚尖</p><p>海风在耳边轻轻诉说</p><p>眼前蔓延的青蓝色</p><p>睁开眼那是多么耀眼的光芒</p><p>至今只在书中领略过的光彩</p><p>无法估计的色温量</p><p>深深印在眼底的圆影</p><p>xxx</p><p>唯有海浪知晓那从未被翻过的扉页</p><p>我们不曾知晓尚未往复重现的历史</p><p>我们并肩仰望的星</p><p>繁星中的某一颗必定是我们的小船</p><p>触及到的这盏灯火</p><p>是多少年月前的产物呢</p><p>浅浅的记忆渐渐流逝</p><p>触及到的全部此刻化作证明</p><p>黑夜最终消散在天空</p><p>带来了全新的黎明</p><p>xxx</p><p>那并不是理所当然</p><p>聆听声音</p><p>停下脚步</p><p>世上竟有如此撼人心弦的景色</p><p>不如上前追问</p><p>即便明知就算询问也不会有答案</p><p>xxx</p><p>我们又能否找到呢</p><p>就像是掬起一片小小的希望般——</p>]]></content>
      
      
      <categories>
          
          <category> 收录集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【写在前面】</title>
      <link href="/2023/10/11/preface-my-daliy-life-xie-zai-qian-mian/"/>
      <url>/2023/10/11/preface-my-daliy-life-xie-zai-qian-mian/</url>
      
        <content type="html"><![CDATA[<h1 id="【写在前面】前言"><a href="#【写在前面】前言" class="headerlink" title="【写在前面】前言"></a>【写在前面】前言</h1><hr><p>自从上了大学之后，越来越觉得自己的懒癌愈发难以医治。每天的日常片段除了手机里混杂着各种信息的网络帖子，剩下的也就是游戏、漫画之类娱乐手段。虽然正经的学业、工作也有在努力，但思来想去，总觉得这样的日常生活实在缺乏乐趣。总得给自己找点儿什么有意思的事，丰富一下自己的生活色彩吧？</p><p>抱着这样的想法，我就搭建了这个属于自己的博客网页。每当自己突然来点儿什么小感悟，或者干出来什么有意思的事情的时候，就用文字把它们记录下来放到这里。就像初高中的时候大家总有那么一两个小笔记本，上面涂涂画画几页勾线都是自己的心路痕迹。</p><p>当然，这样乍一听可能有点驴唇不对马嘴：“明明是说自己的生活缺少滋味，和博客又有什么关系？”，其实写博客就是调理生活的调味剂吧，常记录自己的经历与思考，自然也就能更多地发现那些值得品味的地方。况且有了这个念想在，也就会更多地在生活中发掘自己的思考——“这件事&#x2F;这个想法也许可以记下来呢”，我常常会有这样的念头冒出来。</p><p>回想起高中那会儿，那时候的自己还有点文学情怀（虽然各类文学作品都不甚了解，顶多算找到点儿文字乐趣），加上高中语文老师的要求，就有一个所谓“积累本”，用来记录整理从各个途径里获得的那些有价值的文字。想来语文老师的意思是让我们积累些作文素材，以便填补些高考时肚子里的墨水来用。不过我倒是直接把它当成了类似日记本一样的存在，那时候喜欢文字，喜欢文学，也喜欢自己写一些东西，就索性直接也把自己写的东西都放在上面。</p><p>那可能就是我最初的“个人博客”了吧，好像还曾经被语文老师表扬过呢，嘿嘿。</p><p>总而言之，我想是时候留下一点痕迹供以后的自己翻找回味啦。随着年龄增长也越发体会到时间流逝的加速，今年与明年也往往不再是同样的长度。这个空间不保留完美的人生范本，只收集属于自己的原生态切片。希望以后的自己能够坚持在这里留下印记：</p><ul><li><strong>记录平凡时光里的微尘闪烁</strong></li><li><strong>保存思想潮汐的碰撞与涨落</strong></li><li><strong>晾晒成功或失败的结果交错</strong></li></ul><p>这些文字既是写给自己的路标，也企盼着它能荡起别处某个人心里的涟漪。我常以为这世间的一切种种归根结底都是情感的传递，若某个句子也激起了你的共鸣，欢迎留下你的回声。期待着我们之间有一个共时性的瞬间绽放。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 感悟 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
