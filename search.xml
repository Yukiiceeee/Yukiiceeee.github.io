<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【机器学习】002_线性回归模型Part.1</title>
      <link href="/2023/10/09/ji-qi-xue-xi-002-xian-xing-hui-gui-mo-xing-part.1/"/>
      <url>/2023/10/09/ji-qi-xue-xi-002-xian-xing-hui-gui-mo-xing-part.1/</url>
      
        <content type="html"><![CDATA[<h1 id="【机器学习】002-线性回归模型Part-1"><a href="#【机器学习】002-线性回归模型Part-1" class="headerlink" title="【机器学习】002_线性回归模型Part.1"></a>【机器学习】002_线性回归模型Part.1</h1><hr><h2 id="一、线性回归模型的概念"><a href="#一、线性回归模型的概念" class="headerlink" title="一、线性回归模型的概念"></a>一、线性回归模型的概念</h2><h6 id="1-什么是线性？"><a href="#1-什么是线性？" class="headerlink" title="(1) 什么是线性？"></a>(1) 什么是线性？</h6><p>· 线性：在二元的直角坐标系中，自变量与因变量之间的关系以一条直线表示，因而为“线性”。</p><p>· 例：已知汽车的剩余油量，则其可以行驶多长时间？</p><h6 id="2-什么是回归？"><a href="#2-什么是回归？" class="headerlink" title="(2) 什么是回归？"></a>(2) 什么是回归？</h6><p>· 例：父代身高与子代身高之间的关系。</p><p>· 子代的身高有向族群平均身高“回归”的趋势。</p><p>· 因变量为连续型变量：预测用户收入、预测员工通勤距离等。</p><p><strong>(3) 什么是回归分析？</strong></p><p>· 回归分析着重寻求<strong>变量之间近似的函数关系（线性回归即寻求线性函数关系）。</strong></p><p><strong>(4) 什么是线性函数？</strong></p><p><strong>一元线性函数：</strong><img src="https://latex.csdn.net/eq?f(x)%20=%20w_%7B0%7D%20&plus;%20w_%7B1%7Dx" alt="f(x) = w_{0} + w_{1}x"></p><p><strong>多元线性函数：</strong><img src="https://latex.csdn.net/eq?f(x_%7B1%7D,x_%7B2%7D,...x_%7Bn%7D)%20=%20w_%7B0%7D&plus;w_%7B1%7Dx_%7B1%7D&plus;w_%7B2%7Dx_%7B2%7D&plus;...&plus;w_%7Bn%7Dx_%7Bn%7D%20=%20w_%7B0%7D&plus;%20%5Csum_%7Bi=1%7D%5E%7Bn%7Dw_%7Bi%7Dx_%7Bi%7D" alt="f(x_{1},x_{2},...x_{n}) = w_{0}+w_{1}x_{1}+w_{2}x_{2}+...+w_{n}x_{n} = w_{0}+ \sum_{i=1}^{n}w_{i}x_{i}"></p><h2 id="二、线性回归的原理及理论基础"><a href="#二、线性回归的原理及理论基础" class="headerlink" title="二、线性回归的原理及理论基础"></a>二、线性回归的原理及理论基础</h2><p><img src="https://img-blog.csdnimg.cn/0b1e1da1b53945e6b1fdad502a5121e3.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p><img src="https://img-blog.csdnimg.cn/f96a114317f4481d968d22b0cb183448.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p><img src="https://img-blog.csdnimg.cn/87fac8261d724af69960f0b33ba7bc82.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><h2 id="三、线性回归模型"><a href="#三、线性回归模型" class="headerlink" title="三、线性回归模型"></a>三、线性回归模型</h2><p><strong>线性回归模型的假设如下：</strong></p><p>\1. 变量是相互无关的，各变量的作用与其它变量取什么值无关。</p><p>\2. 变量的作用是可以叠加的，公式中各变量是相加的。</p><p><strong>在机器学习领域，线性回归模型记为：</strong></p><p><img src="https://img-blog.csdnimg.cn/b8f7df45679e4755aebff35faec03479.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动">编辑</p><p><strong>则可以统一形式为：</strong></p><p>​                       <img src="https://latex.csdn.net/eq?y%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7Dw_%7Bi%7Dx_%7Bi%7D%20&plus;%20b%20=%20w%5E%7BT%7Dx&plus;b" alt="y = \sum_{i=1}^{n}w_{i}x_{i} + b = w^{T}x+b"></p><p>其中：</p><p>· <img src="https://latex.csdn.net/eq?y" alt="y"> 是预测函数</p><p>· <img src="https://latex.csdn.net/eq?w" alt="w"> 是模型参数</p><p>· <img src="https://latex.csdn.net/eq?x" alt="x"> 是特征输入</p><p>· <img src="https://latex.csdn.net/eq?b" alt="b"> 是偏置量</p><p><img src="https://img-blog.csdnimg.cn/bcc26bf4708f4c5d817589773f5928cb.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>单个训练数据集：<img src="https://latex.csdn.net/eq?(x%5E%7B(i)%7D,y%5E%7B(i)%7D)" alt="(x^{(i)},y^{(i)})">，上标(index)表示这是第 <img src="https://latex.csdn.net/eq?i" alt="i"> 个训练示例。 </p><p><strong>线性回归模型损失函数：</strong></p><p>损失函数用来计算真实值与预测值的误差，一般取：</p><p>​                                <img src="https://latex.csdn.net/eq?L%20=%20%5Cfrac%7B1%7D%7B2%7D(y-%5Cwidehat%7By%7D)%5E2" alt="L = \frac{1}{2}(y-\widehat{y})^2"></p><p>损失函数的一般公式：</p><p>假设数据集有 <img src="https://latex.csdn.net/eq?m" alt="m"> 个训练样本，<img src="https://latex.csdn.net/eq?n" alt="n"> 个特征工程，则平方损失函数公式如下：</p><p>​                    <img src="https://latex.csdn.net/eq?L(w)=%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bj=1%7D%5E%7Bm%7D%5Cleft%20%5B%20y%5E%7B(j)%7D-%5Csum_%7Bi=1%7D%5E%7Bn%7Dw_%7Bi%7Dx_%7Bi%7D%5E%7Bj%7D-b%20%5Cright%20%5D%5E2" alt="L(w)=\frac{1}{2}\sum_{j=1}^{m}\left [ y^{(j)}-\sum_{i=1}^{n}w_{i}x_{i}^{j}-b \right ]^2"></p><p><strong>线性回归模型的梯度下降训练：</strong></p><p><img src="https://img-blog.csdnimg.cn/1cbbf3bfd0444e6a91764d0b32909a2a.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>· <img src="https://latex.csdn.net/eq?%5Calpha" alt="\alpha">：学习率，自己设定</p><p>· <img src="https://latex.csdn.net/eq?x_%7Bi%7D%5E%7B(j)%7D" alt="x_{i}^{(j)}">：第 <img src="https://latex.csdn.net/eq?j" alt="j"> 个样本的第 <img src="https://latex.csdn.net/eq?i" alt="i"> 特征</p>]]></content>
      
      
      <categories>
          
          <category> 学习打卡 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【机器学习】001_监督学习与非监督学习</title>
      <link href="/2023/10/07/ji-qi-xue-xi-001-jian-du-xue-xi-yu-fei-jian-du-xue-xi/"/>
      <url>/2023/10/07/ji-qi-xue-xi-001-jian-du-xue-xi-yu-fei-jian-du-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="【机器学习】001-监督学习与非监督学习"><a href="#【机器学习】001-监督学习与非监督学习" class="headerlink" title="【机器学习】001_监督学习与非监督学习"></a>【机器学习】001_监督学习与非监督学习</h2><hr><h2 id="一、定义与区别"><a href="#一、定义与区别" class="headerlink" title="一、定义与区别"></a>一、定义与区别</h2><h3 id="·-监督学习（Supervised-learning）-："><a href="#·-监督学习（Supervised-learning）-：" class="headerlink" title="· 监督学习（Supervised learning） ："></a>· 监督学习（Supervised learning） ：</h3><p>提供<strong>带有正确结果的训练集</strong>，基于训练集，算法将归纳（generalization）出“如何正确的响应所有可能的输入”，也就是对测试集有一个明确的输出。也称之为”示例学习（learning from examples）”</p><h3 id="·-非监督学习（Unsupervised-learning）"><a href="#·-非监督学习（Unsupervised-learning）" class="headerlink" title="· 非监督学习（Unsupervised learning）:"></a>· 非监督学习（Unsupervised learning）:</h3><p>训练集<strong>没有提供正确结果</strong>，而是<strong>让算法尝试识别不同数据之间的相似性</strong>，从而让有共同特征的数据能够被归类在一起。以统计学的方式实现监督学习也称作”密度估计（density estimation）“</p><h2 id="二、监督学习"><a href="#二、监督学习" class="headerlink" title="二、监督学习"></a>二、监督学习</h2><p>模型训练原理：输入(X) —&gt; 输出(Y)</p><p>※ 通过X和Y对标签来训练模型，模型从这些输入输出学习后，可接受一个全新的X并尝试对应输出</p><p><strong>应用领域举例：</strong></p><ol><li><p>垃圾邮件过滤：电子邮件 —&gt; 垃圾邮件?(0&#x2F;1)</p></li><li><p>语音识别：声音 —&gt; 文字记录</p></li><li><p>机器翻译：英语 —&gt; 中文</p></li><li><p>外观检验：手机图片 —&gt; 缺陷?(0&#x2F;1)</p></li><li><p>……</p></li></ol><h3 id="回归模型："><a href="#回归模型：" class="headerlink" title="回归模型："></a><strong>回归模型：</strong></h3><p><strong>例：根据房子大小预测房价的AI模型</strong><img src="https://img-blog.csdnimg.cn/2326e122cb85458a835f4c23adb92aae.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>——通过直线拟合、曲线拟合或函数拟合等，对预计的输入X（房子大小）做对应的输出Y（房子价格）的预测</p><h3 id="分类模型："><a href="#分类模型：" class="headerlink" title="分类模型："></a><strong>分类模型：</strong></h3><p>预测事物所属的类别：如肿瘤是良性或恶性；图片显示的是猫还是狗等。</p><p><strong>例：预测肿瘤是良性或是恶性</strong></p><p><img src="https://img-blog.csdnimg.cn/a4b0ac7f70244ed5ad0978f2db791195.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>——输入病人年龄与肿瘤大小、厚度等输入值，通过恰当的算法找到一条合理的分界线来对恶性与良性所属的区域进行区分，再用大量数据对模型进行训练，最终使模型获得根据这些输入得出预测结果的能力。</p><blockquote><p>常见的监督学习算法： k-近临算法，决策树，朴素贝叶斯等等。</p></blockquote><h2 id="三、非监督学习"><a href="#三、非监督学习" class="headerlink" title="三、非监督学习"></a>三、非监督学习</h2><p>※ 不试图监督算法：让模型自己读懂输入与输出对应的模式与关联，获取没有标签的数据并尝试将它们自动分组到集群之中。</p><h3 id="聚类模型："><a href="#聚类模型：" class="headerlink" title="聚类模型："></a><strong>聚类模型：</strong></h3><p><strong>例：根据DNA表达性状对不同人群进行区分与聚类</strong></p><p>并没有提前指明有哪些种类、不同群体有哪些特征——即没有事先让机器明白所应该做的聚类类型、目标有哪些。而最终为产出聚类结果所运用的归类方式是模型自己探寻出来的。</p><p><img src="https://img-blog.csdnimg.cn/8110f512f1fe447f924dd57c383a838e.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>——红色、灰色、绿色代表不同性状表达能力的强弱，机器根据这些颜色所反映出群体能力的不同来对整个人群的DNA序列做了聚类。</p><p> <strong>非监督学习的其它模型：异常检测模型、降维等等</strong></p><blockquote><p>常见的非监督学习算法： k-均值聚类算法，谱聚类算法，EM算法，主成分分析。</p></blockquote><p><strong>例题：选择无监督学习的示例：</strong></p><p><img src="https://img-blog.csdnimg.cn/c628d9c70c56419c9beaecef69471902.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>我的答案：BC</p><p>B：使用聚类的模型算法将新闻文章分组到一起。</p><p>C：给予算法一些数据让模型自动发现并细分市场，对其进行进一步的归类。</p><h2 id="四、半监督学习"><a href="#四、半监督学习" class="headerlink" title="四、半监督学习"></a>四、半监督学习</h2><p>※ 训练数据集同时包含有标记样本数据和未标记样本数据。</p><p>​            <img src="https://img-blog.csdnimg.cn/56e4ce39fc6a4981ba7bb59b13ff2bb5.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动">编辑</p><p><strong>原理：</strong>用一小部分标记好的数据先进行初步的模型训练，再投入大量未标记数据来获得整体的模型结果。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p><strong>监督学习、非监督学习、半监督学习</strong>是机器学习的主要算法分支。后续还会学到<strong>强化学习</strong>，日后在系统地对机器学习的相关算法进行学习、实践的过程中，也应当对这几个算法分支的概念、区别等铭记于心。</p>]]></content>
      
      
      <categories>
          
          <category> 学习打卡 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【机器学习】000_机器学习基本概念</title>
      <link href="/2023/10/03/ji-qi-xue-xi-000-ji-qi-xue-xi-ji-ben-gai-nian/"/>
      <url>/2023/10/03/ji-qi-xue-xi-000-ji-qi-xue-xi-ji-ben-gai-nian/</url>
      
        <content type="html"><![CDATA[<h1 id="【机器学习】000-机器学习基本概念"><a href="#【机器学习】000-机器学习基本概念" class="headerlink" title="【机器学习】000_机器学习基本概念"></a>【机器学习】000_机器学习基本概念</h1><hr><h2 id="一、机器学习的概念"><a href="#一、机器学习的概念" class="headerlink" title="一、机器学习的概念"></a>一、机器学习的概念</h2><h4 id="·-定义：机器基于数据，去寻找一个对应的函数"><a href="#·-定义：机器基于数据，去寻找一个对应的函数" class="headerlink" title="· 定义：机器基于数据，去寻找一个对应的函数"></a><strong>· 定义：</strong>机器基于数据，去寻找一个对应的函数</h4><h2 id="二、机器学习的过程："><a href="#二、机器学习的过程：" class="headerlink" title="二、机器学习的过程："></a><strong>二、机器学习的过程：</strong></h2><h5 id="1-写一个带有未知参数的函数表达式（基于研究领域的领域知识）"><a href="#1-写一个带有未知参数的函数表达式（基于研究领域的领域知识）" class="headerlink" title="1. 写一个带有未知参数的函数表达式（基于研究领域的领域知识）"></a><strong>1. 写一个带有未知参数的函数表达式（基于研究领域的领域知识）</strong></h5><p>​                          <strong><img src="https://latex.csdn.net/eq?y%20=%20wx+b" alt="y = wx+b"></strong></p><p>​      其中，<strong>w, b</strong>（偏置）为未知的参数（权重），从数据中学习得到。</p><p>​      <strong>w：weight b:bias</strong></p><p>​      这个带有未知参数的函数表示式就是模型（<strong>Model</strong>），<strong>x</strong>称为<strong>feature</strong></p><h5 id="2-定义损失，定义训练数据的Loss函数"><a href="#2-定义损失，定义训练数据的Loss函数" class="headerlink" title="2. 定义损失，定义训练数据的Loss函数"></a><strong>2. 定义损失，定义训练数据的Loss函数</strong></h5><p>​      <strong>Loss</strong>是以前面提到的两个未知参数为参数的一个函数：<img src="https://latex.csdn.net/eq?Loss(b,w)" alt="Loss(b,w)"></p><p>​      <strong>Loss</strong>函数的作用是衡量这组数据的好坏，具体的机制如下所示：</p><p>​      <img src="https://latex.csdn.net/eq?Loss(0.5k,%201)" alt="Loss(0.5k, 1)">   <img src="https://latex.csdn.net/eq?y%20=%200.5k&plus;x" alt="y = 0.5k+x"></p><p>​      将已知的一个 <img src="https://latex.csdn.net/eq?x" alt="x"> 代入，得到一个预测的 <img src="https://latex.csdn.net/eq?y_%7B1%7D" alt="y_{1}"> —— <img src="https://latex.csdn.net/eq?y_%7B1%7D" alt="y_{1}"> 和真实的值是有一定差距的。</p><p>​      我们可以计算出误差 <img src="https://latex.csdn.net/eq?e_%7B1%7D=%5Cleft%20%7C%20y-y_%7B1%7D%20%5Cright%20%7C" alt="e_{1}=\left | y-y_{1} \right |"> ，同理我们可以算出已知所有 <img src="https://latex.csdn.net/eq?x" alt="x"> 值对应的误差 <img src="https://latex.csdn.net/eq?e_%7Bn%7D" alt="e_{n}">。</p><p>​                 <img src="https://img-blog.csdnimg.cn/c152c54e8ab843ad964c439ea38eb59a.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>​      误差有两种计算方法：</p><p>​      <img src="https://latex.csdn.net/eq?e%20=%20%7Cy-y_%7B_%7B1%7D%7D%7C" alt="e = |y-y_{_{1}}|">  <strong>L is mean absolute error(MAE)</strong> </p><p>​      <img src="https://latex.csdn.net/eq?e%20=%20(y%20-%20y_%7B1%7D)%5E2" alt="e = (y - y_{1})^2">  <strong>L is mean square error(MSE)</strong></p><p>​      如果 <img src="https://latex.csdn.net/eq?y" alt="y"> 和 <img src="https://latex.csdn.net/eq?y_%7B1%7D" alt="y_{1}"> 都是几率分布的，使用交叉熵。</p><p>​      用不同的 <img src="https://latex.csdn.net/eq?w" alt="w"> , <img src="https://latex.csdn.net/eq?b" alt="b"> 计算<strong>Loss</strong>画出等高线图，可以得到如下图<strong>（Error Surface）</strong>所示的结果：</p><p><img src="https://img-blog.csdnimg.cn/93c15fa3f79642c38c2a488741e5c580.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>​      等高线越偏红色系，说明<strong>Loss</strong>值更大，这组的 <img src="https://latex.csdn.net/eq?w" alt="w"> , <img src="https://latex.csdn.net/eq?b" alt="b"> 效果更差；</p><p>​      越偏蓝色系，说明<strong>Loss</strong>值更小，这组的 <img src="https://latex.csdn.net/eq?w" alt="w"> , <img src="https://latex.csdn.net/eq?b" alt="b"> 效果越好；放到模型里得到的预测更精确。</p><h5 id="3-最佳化"><a href="#3-最佳化" class="headerlink" title="3. 最佳化"></a><strong>3. 最佳化</strong></h5><p>​      找到合适的一组 <img src="https://latex.csdn.net/eq?w" alt="w"> , <img src="https://latex.csdn.net/eq?b" alt="b"> , 带到<strong>Loss</strong>函数里面，让计算所得的<strong>L</strong>值最小。</p><p>​      <img src="https://latex.csdn.net/eq?w,b%20=%20arg%20min%20L" alt="w,b = arg min L"></p><p>​      那具体怎么做呢？我们会用到<strong>Gradient Descent</strong></p><p>​     （梯度下降：一种一阶迭代优化算法，用于寻找可微函数的局部最小值。）</p><p>​      先只看 <img src="https://latex.csdn.net/eq?w" alt="w"> 一个变量：</p><p><img src="https://img-blog.csdnimg.cn/8d8104b6f4334c21908028bd90f8382a.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>​      对于不同的 <img src="https://latex.csdn.net/eq?w" alt="w"> , 可以计算出不同的<strong>L</strong>值：</p><p><img src="https://img-blog.csdnimg.cn/2633ccc54db942f4bbe2fcb234609ea5.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>​      <strong>第一步：</strong>随机选取一个点 <img src="https://latex.csdn.net/eq?w_%7B0%7D" alt="w_{0}"> , 计算该点的微分，得到该点斜率；</p><p>​      <strong>第二步：</strong>判断该点斜率：正，减小 <img src="https://latex.csdn.net/eq?w" alt="w">；负，增大 <img src="https://latex.csdn.net/eq?w" alt="w">（改变的幅度由定义的比率数值决定）</p><p>​      ※ 比率数值 <img src="https://latex.csdn.net/eq?%5Ceta" alt="\eta"> 称为“学习率”，是需要自己定义的变量，即<strong>hyperparameters</strong>（超参数）</p><p>​      <strong>第三步：</strong>迭代更新 <img src="https://latex.csdn.net/eq?w" alt="w"> , 直到找到斜率为0的点所对应的 <img src="https://latex.csdn.net/eq?w" alt="w"> 值。</p><p><img src="https://img-blog.csdnimg.cn/ee1e5894ec5841cf8423c695fe66c6a3.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>​                      <img src="https://img-blog.csdnimg.cn/0e7e5ecd547c4a5b8bdc0f79a8985ea6.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>—&gt; 分别求损失L对 <img src="https://latex.csdn.net/eq?w,b" alt="w,b"> 的偏导数，然后再乘以学习率 <img src="https://latex.csdn.net/eq?%5Ceta" alt="\eta">（自定义参数），在原有 <img src="https://latex.csdn.net/eq?w_%7B0%7D" alt="w_{0}"> 的基础上减去 <img src="https://latex.csdn.net/eq?%5Ceta" alt="\eta"> * 偏导数。</p><p><img src="https://img-blog.csdnimg.cn/5f9ea63660bf46589a1ea8e3ea6a6f01.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>​                             <strong>在Error Surface上的过程体现↑</strong></p><h2 id="三、一些专有名词"><a href="#三、一些专有名词" class="headerlink" title="三、一些专有名词"></a>三、一些专有名词</h2><ul><li>模型（model）：计算机层面的认知</li><li>学习算法（learning algorithm），从数据中产生模型的方法</li><li>数据集（data set）：一组记录的合集</li><li>示例（instance）：对于某个对象的描述</li><li>样本（sample）：也叫示例</li><li>属性（attribute）：对象的某方面表现或特征</li><li>特征（feature）：同属性</li><li>属性值（attribute value）：属性上的取值</li><li>属性空间（attribute space）：属性张成的空间</li><li>样本空间&#x2F;输入空间（samplespace）：同属性空间</li><li>特征向量（feature vector）：在属性空间里每个点对应一个坐标向量，把一个示例称作特征向量</li><li>维数（dimensionality）：描述样本参数的个数（也就是空间是几维的）</li><li>学习（learning）&#x2F;训练（training）：从数据中学得模型</li><li>训练数据（training data）：训练过程中用到的数据</li><li>训练样本（training sample）:训练用到的每个样本</li><li>训练集（training set）：训练样本组成的集合</li><li>假设（hypothesis）：学习模型对应了关于数据的某种潜在规则</li><li>真相（ground-truth）:真正存在的潜在规律</li><li>学习器（learner）：模型的另一种叫法，把学习算法在给定数据和参数空间的实例化</li><li>预测（prediction）：判断一个东西的属性</li><li>标记（label）：关于示例的结果信息，比如我是一个“好人”。</li><li>样例（example）：拥有标记的示例</li><li>标记空间&#x2F;输出空间（label space）：所有标记的集合</li><li>分类（classification）：预测是离散值，比如把人分为好人和坏人之类的学习任务</li><li>回归（regression）：预测值是连续值，比如你的好人程度达到了0.9，0.6之类的</li><li>二分类（binary classification）：只涉及两个类别的分类任务</li><li>正类（positive class）：二分类里的一个</li><li>反类（negative class）：二分类里的另外一个</li><li>多分类（multi-class classification）：涉及多个类别的分类</li><li>测试（testing）：学习到模型之后对样本进行预测的过程</li><li>测试样本（testing sample）：被预测的样本</li><li>聚类（clustering）：把训练集中的对象分为若干组</li><li>簇（cluster）：每一个组叫簇</li><li>监督学习（supervised learning）：典范–分类和回归</li><li>无监督学习（unsupervised learning）：典范–聚类</li><li>未见示例（unseen instance）：“新样本“，没训练过的样本</li><li>泛化（generalization）能力：学得的模型适用于新样本的能力</li><li>分布（distribution）：样本空间的全体样本服从的一种规律</li><li>独立同分布（independent and identically distributed，简称i,i,d.）:获得的每个样本都是独立地从这个分布上采样获得的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习打卡 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
